# DAY 1

강사 정보
---
- 최국현
- tang@linux.com

__점심시간:__ 01시 20분부터 02시 20분까지

__쉬는시간:__ 약 10분 교육

- [페이스메이커 github 주소](https://github.com/tangt64/training_memos/tree/main/opensource/pacemaker-101)
- [강의 메모 주소](https://github.com/tangt64/training_memos/blob/main/opensource/pacemaker-101/20230619-memo.md)
- [교재PDF 주소](https://github.com/tangt64/training_memos/blob/main/opensource/pacemaker-101/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-Pacemaker.pdf)
- [판서 주소](https://wbd.ms/share/v2/aHR0cHM6Ly93aGl0ZWJvYXJkLm1pY3Jvc29mdC5jb20vYXBpL3YxLjAvd2hpdGVib2FyZHMvcmVkZWVtLzk3YzhlNTVhMjBhNTRmNGI4NTU2NWVlYjVmM2M0MDFiX0JCQTcxNzYyLTEyRTAtNDJFMS1CMzI0LTVCMTMxRjQyNEUzRF82M2ExMzZmZS01NTc2LTRkNDMtYTgwMS0yNzA0MDBlYWI4NGQ=)

강의 진행 시 필요한 소프트웨어는 가상화 소프트웨어 및 리눅스 ISO이미지 입니다. 하이퍼바이저는 윈도우 10/11 Pro기준으로 하이퍼브이 사용합니다.

- [센트OS 8 스트림]
- [센트OS 9 스트림](https://www.centos.org/download/)
- [로키 9](https://rockylinux.org/news/rocky-linux-9-0-ga-release/)

## 랩 환경

1. 하이퍼브이 기반으로 진행
2. Windows 10/11 Pro 버전(가상화 가속 기능)
3. 리눅스 호스트 기반으로 사용 가능(libvirt기반으로 랩 가능)
4. 버추얼박스, 권장하지 않음(vCPU 문제. 특히 AMD CPU에서 문제가 있음)
5. VMWare Workstation/Player(교육용 라이선스가 불가능)
6. CentOS-9-Stream, Rocky 9(CentOS 8 Stream)

노트북/데스크탑(워크스테이션)
---
CPU: 4 cores
MEM: 16GiB, 권장은 32GiB

가상머신: vCPU: 2, vMEM: 4GiB, vDisk 100GiB x 4

**기본과정은 4대면(2,3대) 충분**
node1: cluster node
node2: cluster node
node3: cluster node + (storage + mgmt(vDisk x 4개 더 추가가))
node4: cluster node + storage + mgmt(vDisk x 4개 더 추가가)

**추후 고급과정**
~ node7

가상 네트워크: 2개 네트워크 사용 예정(default, internal)

## CentOS 8 to Centos 8 Stream

```bash                                
dnf --disablerepo '*' --enablerepo extras swap centos-linux-repos centos-stream-repos
dnf distro-sync
```

## 페이스메이커 소개

https://clusterlabs.org/


루트 권한으로 아래 작업 수행

```bash

nmcli con mod eth1 ipv4.addresses 192.168.90.170/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname eth1 && nmcli con up eth1

ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa
dnf install sshpass -y
cat <<EOF> ~/.ssh/config
StrictHostKeyChecking=no
EOF
for i in {1..4} ; do sshpass -p centos ssh-copy-id root@node${i} ; done
ssh root@node1 "hostname"
ssh root@node2 "hostname"
ssh root@node3 "hostname"
ssh root@node4 "hostname"

for i in node{1..4} ; do sshpass -p centos ssh root@$i 'dnf update -y' ; done
> dnf update -y ## node1,2,3,4
for i in node{1..4} ; do sshpass -p centos ssh root@$i 'dnf --enablerepo=highavailability -y install pacemaker pcs' ; done
> dnf --enablerepo=highavailability -y install pacemaker pcs
> dnf --enabelrepo=resilientstorage -y install pacemaker pcs


for i in node{1..4} ; do sshpass -p centos ssh root@$i 'dnf install firewalld && systemctl enable --now firewalld' ; done
> dnf install firewalld && systemctl enable --now firewalld
for i in {1..4} ; do sshpass -p centos ssh root@node${i} 'firewall-cmd --add-service=high-availability && firewall-cmd --runtime-to-permanent' ; done
for i in node{1..4} ; do sshpass -p centos ssh root@$i 'systemctl stop firewalld' ; done
> systemctl stop firewalld   ## 사용하지 않으시면


for i in {1..4} ; do sshpass -p centos ssh root@node$i 'echo centos | passwd --stdin hacluster && systemctl enable --now pcsd.service' ; done
> echo centos | passwd --stdin hacluster && systemctl enable --now pcsd.service
> hacluster사용자 암호를 "centos"으로 변경
> pcsd서비스는 pcs명령어에서 사용

## FOR CENTOS8/ROCKY8
dnf --enablerepo=highavailability --> dnf --enablerepo=resilientstorage
```

### 설치 과정

pcs명령어는 올바르게 설정 파일 배포를 하기 위해서 "pcsd.service"가 모든 노드에서 동작해야 한다. 

- H/A저장소 활성화 및 pcsd, pacemaker 패키지 설치
- "pcsd.service" 활성화
- 'pcs host auth' 각각 호스트 인증(=pcs deauth host)
- 'pcs cluster setup' 클러스터 구성 시작(=pcs cluster setup --start --enable)
    + pcs cluster stop --all && pcs cluster destroy --all
    + --start: 클러스터 시작(pacemaker)
    + --enable: 서비스 부트업(pacemaker)
- 'pcs status'
- 'pcs cluster status'

특정 노드 제외 순서
- pcs status
- pcs host deauth node7.example.com
- pcs cluster delete node7.example.com

특정 노드 추가
- pcs host auth node7.example.com
- pcs cluster node add node7.example.com 


### 질문들

>pcs cluster stop --all && pcs cluster destroy --all
>Error: Unable to read /etc/corosync/corosync.conf: No such file or directory

>네트워크 상태 확인


>운용중 corosync, pacemaker  disable 상태를 권장한다는 정책을 보았는데 이게 맞나요?

## 명령어


페이스메이커 준비 및 설치. 아래 명령어는 스크립트 제외한 명령어. 스크립트로 처리를 원하는 경우 PPT참고 바람.
```bash
## 옵션으로 저장소 활성화
dnf --enablerepo=highavailability search pacemaker pcs
dnf --enablerepo=highavailability install pacemaker pcs -y     ## 모든 노드에 설치

systemctl is-active pcsd.service
systemctl enable --now pcsd.service


hostnamectl                                                ## 호스트 이름 확인
vi /etc/hosts                                              ## node3번에서 작업중 
192.168.90.110 node1.example.com node1                     ## sda,b,c,d(iscsi)
192.168.90.120 node2.example.com node2                     ## sda,b,c,d(iscsi)
192.168.90.130 node3.example.com node3                     ## 블록장치를 추가 b,c,d + e,f,g(iscsi)
192.168.90.140 node4.example.com node4 storage cli utility ## 파일기반으로 블록장치 구성


sshpass -pcentos scp /etc/hosts node1:/etc/hosts

ping -c2 node{1..3}

ssh-keygen -t rsa -N'' ~/.ssh/id_rsa                     
dnf install sshpass -y                                   
vi  ~/.ssh/config                                         ## fingerprint 무시
StrictHostKeyChecking=no

sshpass -pcentos ssh-copy-id root@node{1..3}              ## node1~3번까지 공개키 전달
sshpass -pcentos scp /etc/hosts node{1..3}:/etc/hosts

## 루트 로그인이 안되시는 경우
/etc/ssh/sshd_config.d/01-rootallow.conf
PermitRootLogin yes                                      

## SSH passphase키 질문 부분
vi /etc/ssh/sshd_config.d/02-keyallow.conf
PubkeyAuthentication yes


systemctl restart sshd

## node 1~3번까지 확인

sshpass -pcentos ssh node1 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node2 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node3 "hostname && grep hacluster /etc/passwd"


grep hacluster /etc/passwd                            ## pacemaker hacluster 사용자, PCS 루트 사용자

## node1~3번까지 패스워드 설정

echo cluster | passwd --stdin hacluster
grep hacluster /etc/passwd
hacluster:x:189:189:cluster user:/home/hacluster:/sbin/nologin               ## pcs명령어 접근 및 실행시 사용

systemctl enable --now pcsd.service                                          ## node1, node2, node3
systemctl stop firewalld                                                     ## 방화벽 중지
pcs host auth -u hacluster -p cluster node1.example.com node2.example.com  node3.example.com  ## node3
> node1.example.com: Authorized
> node2.example.com: Authorized
> node3.example.com: Authorized
#
# /var/lib/pacemaker 각 노드끼리 토큰 인증
#

pcs status
pcs cluster status
pcs cluster setup ha_cluster_lab node1.example.com node2.example.com node3.example.com
                  -------------  ----------------- -----------------
                  클러스터 이름      노드 이름         노드 이름       .......
--> 만약에 이전 내용이 존재하면 제거를 시도
--> pcs == pcsd == pacemaker
--> corosync == quorum
pcs cluster start --all
pcs cluster enable --all 

pcs cluster status
pcs status

## pcs ---> pcsd.service
ls -laR /etc/corosync
ls -laR /etc/pacemaker
ls -laR /var/lib/corosync
ls -laR /var/lib/pacemaker
```

### target서버(iscsi)

```bash
node4# dnf install targetcli -y
node4# systemctl enable --now target
node4# firewall-cmd --add-service=iscsi-target
node4# dnf install iscsi-initiator-utils -y
node4# mkdir -p /var/lib/iscsi_disks/
node4# targetcli backstores/fileio create sdb /var/lib/iscsi_disks/sdb.img 2G
node4# targetcli backstores/fileio create sdc /var/lib/iscsi_disks/sdc.img 2G
node4# targetcli backstores/fileio create sdd /var/lib/iscsi_disks/sdd.img 2G
node4# targetcli iscsi/ create iqn.2023-02.com.example:blocks
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdb/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdc/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdd/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node1.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node2.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node3.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node4.init
node4# targetcli saveconfig
```

node 1/2/3/4번에 아래 작업 수행
```bash
nodeX# dnf install iscsi-initiator-utils -y
nodeX# cat <<EOF> /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2023-02.com.example:node1.init

nodeX# systemctl restart iscsi iscsid
nodeX# iscsiadm -m discovery -t sendtargets -p 192.168.90.140
nodeX# iscsiadm -m node --login
```

최종 결과물
```bash
lsblk
```

## 창인님+주용님 여기 보셔용 :)

PPT: 65, 66, 70, 71, 72, 73, 74, 75(뒤에 stop, destroy는 하지 마세요),77 + 메모파일의 [target 서버](#target%EC%84%9C%EB%B2%84iscsi)


# DAY 2

## 질문 :)

>connection1:0: detected conn error (1020)

해당 오류는 iscsid에서 프로토콜 설정 값 문제로 특정 기능 연결이 불가한 상태
>lsblk
>iscsiadm -m session

## 준비사항

pcs명령어를 계속 사용해야됨. 좀 더 명령어를 덜 입력하는 방법.

1. bash completion(권장)
2. zsh
3. fish(POSIX SH지원 안함)

```bash
dnf search bash-completion
dnf install bash-completion -y
complet -r -p
exit | bash       ## 둘중 하나 실행해서 쉘 환경 갱신 혹은 재구성
```

```bash
dnf --enablerepo=highavailability -y install fence-agents-scsi
ls /dev/disk/by-id/
pcs stonith create scsi-shooter fence_scsi pcmk_host_list="node1.example.com node2.example.com node3.example.com node4.example.com " devices=/dev/disk/by-id/wwn-0x600140557d87dc6dd81446788e00c5fb meta provides=unfencing         ## 노드 상관없이 해당 명령어 실행하면 모든 노드에 동일한 fence agent 복제 및 구성
pcs stonith status
pcs stonith config scsi-shooter
pcs stonith fence nodeX.example.com
node2> systemctl is-active corosync pcsd pacemaker
node2> reboot
node4> pcs cluster start --all                   ## 이렇게 하여도 복구 됨.
```

## 질문 :)

1. 모든 노드에 리소스 구성(결론, 그때그때 다름)
>선택사항. 모든 노드에 리소스 구성시 두 가지 사항을 고려
>- 디스크: 단일 디스크 형태 혹은 공유 파일 시스템 기반으로 구성
>- 서비스 유형: 클러스터 기능이 없는 솔루션에 클러스터 적용
2. 리소스 노드 선택 가능여부
>가능.
>node1, node2, node3, node4, node5,
>RES1(httpd(PREFER{node3, node5, node6}))
>RES2(mysql(PREFER{node4, node1}))
>RES3(tomcat(PREFER{ALL}))
>stickness, prefer, colocate, group(resource group)
3. 자원 산정 2배수(모든 노드가 실패한다는 조건하에)
>구성 및 구축할 클러스터는 노드 5개가 구성. 이중 한대는 lastman-stading으로 구성
>node1,2,3,4은 일반적인 서비스 노드(배수 1.5배(메모리 위주))
>node5 lastman-standing, node 1,2,3,4가 전부 종료가 되었을때 모든 서비스를 가져가는 슈퍼노드(CPU/MEM x 5배수 + 실제 가동시 사용하는 데이터메모리)

4. SAN기반 공유스토리지 볼륨에서 엑티브만 보통 접근. 좀전의 설명은 엑티브-엑티브?
>둘다 적용 가능. 
>엑티브-엑티브: 파일 시스템이 다중 접근 지원(GFS2,btrfs,moonfs)
>엑티브-패시브: ext4, xfs기존 파일시스템 기반으로 구성(drbd으로 권장)

5. 모든 노드가 전부 서비스를 하는 경우 동일한 애플리케이션 서비스인가?
>모든 노드에 동일한 애플리케이션 서비스도 가능.
>특정 노드별로 각기 다른 서비스 구성도 가능.
>
## 과정외 추가 대모

저장소 미러링
---
- appstream
  + 저장소가 stream형식으로 바뀌면서 모듈도 지원
- baseos
- ha

```bash
dnf repolist dnf repolist --all                     ## /etc/yum.repos.d/centos-addons.repo 조회
mkdir -p /home_lvmdirectory/repo-ha

dnf reposync --repoid=highavailability
dnf install createrepo_c -y
createrepo ha/
createrepo re/

dnf module list                                     ## appstream 저장소
```

# DAY 3

## 진행 전, 해야될 사항

1. 저장소 미러링 방법
2. 연습문제 진행 및 풀이(설명)

## 질문

>Error: error running crm_mon, is pacemaker running? 
>crm_mon: Connection to cluster failed: Connection refused

페이스메이커 서비스가 올바르게 미동작. 
- pcs cluster start 
- "cluster setup"


__LVM2__: 로컬
DLM/cLVM: 메타정보 관리. 레드햇은 DLM기반으로 구성을 권장. DLM=GFS2, cLVM--->drbd
>LVM Manager
- https://documentation.suse.com/sle-ha/12-SP5/html/SLE-HA-all/cha-ha-clvm.html

>Distributed Lock Manager (DLM)
>Coordinates disk access for cLVM and mediates metadata access through locking.
>
>Logical Volume Manager2 (LVM2)
>Enables flexible distribution of one file system over several disks. LVM2 provides a virtual pool of disk space. 
>
>Clustered Logical Volume Manager (cLVM)
>Coordinates access to the LVM2 metadata so every node knows about changes. cLVM does not coordinate access to the shared data itself; to enable cLVM to do so, you must configure OCFS2 or other cluster-aware applications on top of the cLVM-managed storage. 

```bash
pcs resource create dlm ocf:pacemaker:controld op monitor interval=30s on-fail=fence
pcs resource clone dlm clone-max=2 clone-node-max=1 interleave=true ordered=true
pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s on-fail=fence
pcs resource clone clvmd clone-max=2 clone-node-max=1 interleave=true ordered=true

dnf install --enablerepo=resilientstorage,highavailability dlm lvm2-lockd
```
cLVM == DRBD(AA, AS)

1. SUSE경우 SBD STONITH가 A-S 양 Node에 대해서 각각 구성되는 형태로 Reference Doc.에서 확인되는데, RHEL은 Cluster Member 중 하나의 Node에만 올라와 있으면 되는 것인지
>둘다 같은 HA소프트웨어.
2. 두 Node중 하나에서만 작동해도 된다면, 2-Node A-S Cluster에서 SBD 기반으로 STONITH 구성 시, 해당 STONITH resource가 Master(Active) Node상에서 작동해야 하는지, Standby Node상에서 작동해야 하는지
>모든 노드에 SDB구성이 필요 합니다. AS/AA상관 없이.

## target서버에 디스크 추가

```bash
node4# targetcli backstores/fileio create sdX /var/lib/iscsi_disks/sde.img 2G
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdX/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:nodeX.init
```

## LVM2기반 공유 디스크 구성 시 주의 사항

__san switch__: sanlock

__software lock__: dlm

```bash
vgcreate -Ay 
vgcreate --help
 -c|--clustered: clvm(lvmlockd)
 --locktype: dlm, 기본값은 none. 만약, san switch를 통해서 받는 경우 sanlock.
 --shared: with(locktype+lvmlockd(clvm(vg_hex_lock)))
man vgcreate 

vgcreate --shared --locktype dlm vg_ha_lvm /dev/sdb1
vgcreate -Ay -cy --shared --locktype dlm vg_ha_lvm /dev/sdb1
```

## 내일 강의 준비

- CENTOS/ROCKY기반으로 1/2/3/4에 서비스 구성
- NODE1/2/3/4를 "ROCKY LINUX 9"으로 재설치
- 일반 LVM2강제로 다른 노드에서 인식
- drdb는 왜 centos-9-stream에서 안되는건가?
- 클러스터 구성 및 설치

# DAY 4

## 설치 방법 간단하게 정리

1. 호스트 네임 설정(/etc/hosts)

아이피로 서버 구성하시는 경우 아래 작업이 필요 없음. pacemaker메뉴얼에는 도메인 기반으로 구성 권장.

```bash
hostnamectl set-hostname node1.example.com
vi /etc/hosts
```

2. ssh private/public(ssh-keygen, ssh-copy-id)

이 명령어는 옵션. 

3. hacluster

pcsd가 사용하는 계정. 
```bash
grep hacluster /etc/passwd
echo centos | passwd --stdin hacluster
```

4. pcsd서비스 

pcs명령어는 pcsd를 통해서 pacemaker, corosync에 전달. 

```bash
systemctl start pcsd.service && systemctl enable pcsd.service
systemctl enable --now pcsd.service
```

5. 패키지

아래 패키지는 필수로 설치가 되어 있어야 됨.

```bash
dnf install pacemaker pcs -y 
```
6. 클러스터 구성

```bash
pcs host auth <IP_ADDRESS> <FQDN>
pcs cluster setup cluser_lab <IP_ADDRESS> <FQDN> -u <USERNAME> -p <PASSWORD> --enable --start
pcs cluster status
```

7. LVM2 확장 명령어

cluster 용도로 LVM2구성을 하기 위해서 미리 해당 파일 수정 및 배포.

```bash
vi /etc/lvm/lvm.conf
use_lvmlockd = 1
system_id_source = uname          ## 앞으로는 machineid

systemctl restart lvmlockd dlm
```

```bash
pvcreate /dev/sdb
vgcreate --shared --locktype dlm vg_ha_lvm /dev/sdb
lvcreate -l 100%FREE -n lv_ha_lvm vg_ha_lvm
```
8. Cluster LVM2기반으로 강제인식 시키는 방법

```bash
lvm pvscan
vgchange vg_ha_lvm -an
lvm pvscan
lvmdevices --adddev /dev/sdb         ## shared가 아니어도 강제로 인식 가능
vgs
lvs
```

9. centos에서 drbd모듈 사용하기.

```bash
## From Rocky Linux
dnf download kernel-core kernel-modules-core kmod-drbd9x
scp * <CENTOS_SRV>

## To CentOS
rpm -ivh --force *.rpm
[SECURE_BOOT_DISABLED]
```

## 질문

노드 강제??

```bash
pcs constraint                ## pcs resource move, pcs constraint locate
Location Constraints:
  Started resource 'tomcat' prefers node 'pmnode02.example.com' with score
      INFINITY
Colocation Set Constraints:
  Set Constraint:
    score=INFINITY
    Resource Set:
      Resources: 'tomcat_service', 'tomcat_vip'
```


## 아파치

resource구성 시 주의하실 점.

1. 테스트로 사용했던 systemd service 꼭 끄셔야 합니다.
2. instance(process)기반으로 실행 후 문제 발생.