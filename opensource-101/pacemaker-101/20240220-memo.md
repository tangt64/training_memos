# DAY 1

강사 정보
---
- 최국현
- tang@linux.com

__점심시간:__ 12:00 ~ 13:20+10분까지

__쉬는시간:__ 15분

- [페이스메이커 github 주소](https://github.com/tangt64/training_memos/tree/main/opensource/pacemaker-101)
- [강의 메모 주소](https://github.com/tangt64/training_memos/blob/main/opensource/pacemaker-101/20240220-memo.md)
- [교재PDF 주소](https://github.com/tangt64/training_memos/blob/main/opensource-101/pacemaker-101/101-%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-Pacemaker.pdf)
- [판서 주소](https://wbd.ms/share/v2/aHR0cHM6Ly93aGl0ZWJvYXJkLm1pY3Jvc29mdC5jb20vYXBpL3YxLjAvd2hpdGVib2FyZHMvcmVkZWVtLzIyOTk4OGIzYTNlYTQxZWY5MmU3MzgyZmFkZTc0YjY1X0JCQTcxNzYyLTEyRTAtNDJFMS1CMzI0LTVCMTMxRjQyNEUzRF9lYzczY2I0ZS01OGM1LTRiNTAtYTU3My05ODVhMjA2OTk4NTY=)

강의 진행 시 필요한 소프트웨어는 가상화 소프트웨어 및 리눅스 ISO이미지 입니다. 하이퍼바이저는 윈도우 10/11 Pro기준으로 하이퍼브이 사용합니다.

- [센트OS 9 스트림](https://www.centos.org/download/)
- [로키 9](https://rockylinux.org/news/rocky-linux-9-0-ga-release/)

## 랩 환경

x86_64bit/AMD
---
1. VMware Workstation/Player: 라이선스 문제
2. VirtualBox: vCPU버그 문제 및 네트워크
3. Hyper-V: Windows 10/11 Pro

가상머신
---
- vCPU: 2
- vMEM: 2048MiB, Recommend, 4096MiB
- Node1/2/3: 20GiB
- Node4: 150GiB(targetd server)

만약, 노드를 4대 이상 생성이 어려운 경우는 아래와 같이 생성
- node1/2/3
- node3번이 iSCSI서버가 됨


랩 환경 조건
---
1. 하이퍼브이 기반으로 진행
2. Windows 10/11 Pro 버전(가상화 가속 기능)
3. 리눅스 호스트 기반으로 사용 가능(libvirt기반으로 랩 가능)
4. 버추얼박스, 권장하지 않음(vCPU 문제. 특히 AMD CPU에서 문제가 있음)
5. VMWare Workstation/Player(교육용 라이선스가 불가능)
6. CentOS-9-Stream, Rocky 9(CentOS 8 Stream)


로키 리눅스 사용하지 않는 이유
---
1. pacemaker 및 pcsd의 패키지가 조금 달라짐.
2. 로키 리눅스는 레드햇 라이선스 정책상 SRPM를 그대로 사용이 불가능.
3. fence(stonith)장치가 올바르게 동작하지 않음.(라이브러리 링크가 다름)
4. centos-9-stream기반으로 진행.

"eth0"네트워크 정적으로 변경(static dhcp)
---

```powershell
powershell> New-VMSwitch -SwitchName "Outworld Switch" -SwitchType Internal
powershell> Get-NetAdapter
> ifIndex, <인덱스 번호>
powershell> New-NetIPAddress -IPAddress 192.168.0.1 -PrefixLength 24 -InterfaceIndex 17
powershell> New-NetNat -Name MyNATnetwork -InternalIPInterfaceAddressPrefix 192.168.0.0/24
```


"root"계정으로 로그인 후, 다음처럼 아이피 변경.

```bash
nmcli con sh
# node1, 나머지 노드들은 뒤에 아이피만 변경
nmcli con mod eth0 ipv4.addresses 192.168.0.110/24 ipv4.gateway 192.168.0.1 ipv4.dns 8.8.8.8 ipv4.method manual
nmcli con up eth0

```


노트북/데스크탑(워크스테이션)
---
CPU: 4 cores
MEM: 16GiB, 권장은 32GiB

가상머신: vCPU: 2, vMEM: 4GiB, vDisk 100GiB x 4

**기본과정은 4대면(2,3대) 충분**
node1: cluster node
node2: cluster node
node3: cluster node + (storage + mgmt(vDisk x 4개 더 추가가))
node4: cluster node + storage + mgmt(vDisk x 4개 더 추가가)


# DAY 1


기본 네트워크(Default, NAT)
---
DHCP하이퍼브이에서 제공. 아이피가 하루에 한번 릴리즈가 됨.

정적 네트워크(Static Switch, NAT)
---
DHCP사용하지 않고, 아이피를 고정으로 구성.

내부 네트워크(Internal Switch)
---
외부와 통신이 되지 않고, 내부망으로만 사용.


### 노드 설정

호스트 네임 및 /etc/hosts설정
---

node1/2/3/4에 올바르게 호스트이름 설정이 되었는지 확인.
```bash
hostnamectl
> Static hostname: node1.example.com
hostnamectl set-hostname node1.example.com
cat <<EOF>> /etc/hosts
192.168.90.110 node1.example.com node1
192.168.90.120 node2.example.com node2
192.168.90.130 node3.example.com node3
192.168.90.140 node4.example.com node4
EOF
cat /etc/hosts
```

네트워크 설정 확인
---

모든 노드에서 __"eth1"__ 아이피 주소 확인.
```bash
ip a s eth1
```

SSH 키 생성(관리용)
---
__"node1"__ 번 에서만 실행 및 구성
```bash
ssh-keygen -t rsa -N '' 
dnf install -y sshpass 
cat <<EOF> ~/.ssh/config
StrictHostKeyChecking=no
EOF
for i in {1..4} ; do sshpass -p centos ssh-copy-id root@node${i} ; done
```
```bash
Static Swithc -> Default Switch
[콘솔 열기]
nmcli con up eth0
ip a s eth0
nmtui edit eth0
> DHCP, 나머지 제거
nmcli con up eth0
ip a s eth0
ping 
```

pcs/pacemaker설치
---
```bash
for i in node{1..4} ; do ssh root@$i 'dnf --enablerepo=highavailability -y install pacemaker pcs' ; done
dnf --enablerepo=highavailability -y install pacemaker pcs
```

방화벽 및 서비스 
---
```bash
node1# for i in {1..4} ; do ssh root@node${i} 'firewall-cmd --add-service=high-availability && firewall-cmd --runtime-to-permanent' ; done
node1# for i in {1..4} ; do ssh root@node$i 'echo centos | passwd --stdin hacluster && systemctl enable --now pcsd.service' ; done
```

클러스터 인증 및 생성
---
```bash
node1# pcs host auth -u hacluster -p centos node1.example.com node2.example.com node3.example.com 

node1# pcs cluster setup ha_cluster_lab node1.example.com node2.example.com node3.example.com --start --enable

node1# pcs status
```

테스트! 테스트!
---
```bash
node1# pcs resource create dummy1 ocf:pacemaker:Dummy
node1# pcs resource create vip ipaddr2 ip=192.168.90.250 cidr_netmask=24
node1# pcs resource delete dummy1
node1# pcs resource delete vip
```

# DAY 2

## DRBD 랩


```bash
node1/2# dnf install epel-release -y
node1/2# dnf search drbd
> drbd-pacemaker.x86_64 : Pacemaker resource agent for DRBD
> drbd-rgmanager.x86_64 : Red Hat Cluster Suite agent for DRBD
node1/2# dnf install drbd drbd-bash-completion drbd-pacemaker drbd-utils -y

node1/2# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org
node1/2# yum install https://www.elrepo.org/elrepo-release-9.el9.elrepo.noarch.rpm -y

node1/2# dnf search drbd
node1/2# dnf install kmod-drbd9x -y
node1/2# dnf install kmod-drbd84 -y     ## 이게 기본값

node1/2# dnf install kmod-drbd84 drbd drbd-utils -y
node1/2# dnf install kernel-abi-stablelists kernel -y && reboot 

node1/2# modprobe drbd
node1/2# modinfo drbd 
> version: 
node1/2# ls -l /etc/depmod.d
> drbd~
node1/2# ls -l /etc/modprobe
> drbd options

node1/2# systemctl enable --now drbd
node1/2# 

node1/2# dnf install --enablerepo elrepo-kernel kernel-ml -y && reboot
```

1. drbd
오픈소스(공개) 버전의 drbd-core 패키지.
```bash
rpm -ql drbd
rpm -ql drbd-utils
```
2. drbd-pacemaker
3. drbd-bash-completion

### 디버깅

```bash
journalctl -fl -perr -pwarning _COMM=pacemaker-fence

journalctl -fl -perr -pwarning -upacemaker

pcs resource cleanup

pcs stonith history cleanup

cd /var/lib/iscsi/send_targets
> send_targets/
> nodes/

```




```bash
fence_ilo4_ssh -a 192.168.1.100 -x -l hpilofence -p hpilopass -o status 
fence_ilo4_ssh -a 192.168.1.101 -x -l hpilofence -p hpilopass -o status

pcs stonith create server105-cpn_ilo4_fence fence_ilo4_ssh ipaddr="192.168.1.100" login="hpilofence" secure="true" passwd=hpilopass  pcmk_host_list="server105-cpn" delay=10 op monitor interval=60s

pcs stonith create server106-cpn_ilo4_fence fence_ilo4_ssh ipaddr="192.168.1.101" login="hpilofence" secure="true" passwd=hpilopass  pcmk_host_list="server106-cpn" op monitor interval=60s

pcs stonith show --full

```

# DAY 3

drbd
---
- 커널 버전보다는, 릴리즈 시 drbd기능이 활성화 되어 있는지
- 블록 복제가 주요 목적
- 재해복구 목적으로 만들어 짐
- 멀티 노드를 지원은 하나, 접근 단일 노드만 가능
- DRBD 프록시 서비스를 통해서 확장 기능 제공

페이스메이커
---
- libQB, 특정 상황에 대해서 판단.
- "corosync.service", 토템(Totem) 클러스터에서 구성된 노드들 상태 확인(heartbeat)
- "pcs.service", pcs명령어에 대한 서비스 데몬
  + "crm.service", crm명령어
  + "crm", "pcs" 둘 다 표준 명령어
- "pacemaker.service", 특정 상황에서 어떻게 동작할지 결정
- resource(monitoring)
  + 서비스를 모니터링하는 에이전트 프로그램
  + 특정노드 혹은 클러스터에서 자원을 생성 및 종료
  + .service(systemd)
  + shell script(OCF Script, LSB)
- stonith(fence agent)
  + 노드 차단 조건(node fencing)
  + 디스크나 혹은 네트워크 혹은 메모리와 같은 장치
  + NODE_FENCE((DIKS | NETWORK | MEMORY)(TRUE))

## BOOTH

>Typically, multi-site environments are too far apart to support synchronous communication and data replication between the sites. That leads to significant challenges:
>
부스의 주요 목적은 각 다른 사이트(지역)에 구성된 클러스터를 하나로 동기화 하는 기능.

- How do we make sure that a cluster site is up and running?
  + 얼마나 많은 클러스터가 사이트에서 동작하는가? 
- How do we make sure that resources are only started once?
  + 리소스 부분. 서비스 시작시 리소스 할당 부분.
- How do we make sure that quorum can be reached between the different sites and a split-brain scenario avoided?
  + A사이트, B사이트가 각기 다른 쿼럼(정족수)가 도달하였을때 어떻게 S/B를 해결할것인가?
- How do we manage failover between sites?
  + 각 사이트의 장애 처리?
- How do we deal with high latency in case of resources that need to be stopped
  + 지연부분(레이턴시)

DR
---
```bash
node1# pcs host auth -uhacluster -pcentos node1.example.com node2.example.com node3.example.com node4.example.com
node1# pcs cluster setup none-dr-nodes node1.example.com node2.example.com --start --enable
node1# pcs cluster setup dr-nodes node3.example.com node4.example.com --start --enable
node1# pcs dr set-recovery-site node3.example.com
node1# pcs dr config
node1# pcs dr status
```

resource
---

1. pcs명령어 입력 순서대로 자원을 구성.
2. 리부팅 이후에는 입력된 순서대로 동작 및 구성이 되지 않음.
3. location를 통해서 리소스 위치를 명시.
4. 동작 순서를 명시하기 위해서는 order 지시자를 사용.


node1
---
```bash
pcs resource create demo-resource ocf:pacemaker:Dummy
pcs resource
```
node2
---
```bash
pcs resource create demo-resource-node2 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node2 prefers node1.example.com=100
pcs resource
pcs constraint config --full
pcs constraint delete location-demo-resource-node2-node1.example.com-100
```


연습용 자원생성
---

```bash
pcs resource create demo-resource-node1 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node1 prefers node1.example.com=100
pcs resource create demo-resource-node2 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node2 prefers node2.example.com=100
pcs resource create demo-resource-node3 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node3 prefers node3.example.com=100
pcs resource create demo-resource-node4 ocf:pacemaker:Dummy
pcs constraint location demo-resource-node4 prefers node4.example.com=100
```

standby
---
```bash
pcs node standby node1.example.com
pcs resource
```

maintance
---

```bash
pcs node maintenance node2.example.com 
pcs resource
pcs resource create demo-resource-node2-maintenance ocf:pacemaker:Dummy
pcs constraint location demo-resource-node2-maintenance prefers node2.example.com=100
pcs node unmaintenance node2.example.com

pcs property set maintenance-mode=true
pcs property set maintenance-mode=false
```

corosync engine
---
1. knet, corosync
2. qdevice(ffsplit, 2nodelms, lms)
3. qdevice(ffsplit, lms)

Quorum,Quorate, QDevice(쿼럼 관련된 자원)
---
1. 최소 3개 혹은 이상의 노드.
2. 각 노드에는 투표 권한을 가지고 있음. 
3. DC에 일정 주기별로 투표를 함.
4. 투표를 제 시간에 하지 못한 경우, 차단.
5. Qnet장치를 별도 구성도 권장.
6. QDevice를 사용해서 분리하는 경우, 독립 노드에서 구성 권장.


1. resource: 모니터링 혹은 하트비트 에이전트(systemd의 .service)
2. pacemaker: 페이스메이커 관련된 자원 에이전트.
3. openstack
4. systemd: "/lib/systemd/system/"에 있는 자원을 제어.


order
```
resource.heartbeat(monitoring) --> systemd(apache.service)
       ---------
       \
        `---> Port, Socket
              URL
```

```bash

```



```bash
grep system_id_source /etc/lvm/lvm.conf
> # Configuration option global/system_id_source.
> system_id_source = "uname"        ## 이 내용 변경
> # This is used when system_id_source is set to 'file'.
for i in {2..4} ; do scp /etc/lvm/lvm.conf root@node${i}:/etc/lvm/lvm.conf ; done
parted --script /dev/sdb "mklabel gpt"
parted --script /dev/sdb "mkpart primary 0% 100%"
parted --script /dev/sdb "set 1 lvm on"

vgcreate --setautoactivation n vg_ha_lvm /dev/sdb1
lvcreate -l 100%FREE -n lv_ha_lvm vg_ha_lvm
vgs -o+systemid 

pvs && vgs && lvs

mkfs.xfs /dev/vg_ha_lvm/lv_ha_lvm

node2/3/4# lvm pvscan --cache --activate ay
node2/3/4# lvmdevices --adddev /dev/sdb1 
node2/3/4# lsblk

pcs resource create lvm_ha_iscsi LVM-activate vgname=vg_ha_lvm vg_access_mode=system_id --future group ha_lvm_group
>[root@node1 ~]# pcs resource
>  * Resource Group: ha_lvm_group:
>    * lvm_ha_iscsi      (ocf:heartbeat:LVM-activate):    Started node1.example.com
pcs resource create lvm_ha_mount FileSystem device=/dev/vg_ha_lvm/lv_ha_lvm directory=/home/lvm_directory fstype=xfs --future group ha_lvm_group
```


Alert Email
---

```bash

```

ILO Stonith
---

```bash

```

Read Only Account
---

```bash

```

## DAY 4

기본 펜싱 장치가 사라진 경우 아래 명령어.

```bash
node1# for i in {1..4} ; do ssh root@node${i} "dnf install --enablerepo=highavailability fence-agents-all watchdog -y" ; done
node1# for i in {1..4} ; do scp /usr/share/cluster/fence_scsi_check root@node${i}:/etc/watchdog.d/ && systemctl enable --now watchdog ; done
node1# ls -l /dev/disk/by-id
node1# pcs stonith create scsi-shooter fence_scsi pcmk_host_list="node1.example.com node2.example.com node3.example.com node4.example.com" devices=/dev/disk/by-id/wwn-<ID> meta provides=unfencing 
node1# pcs stonith config scsi-shooter 
node1# pcs status
```

__"node4" 타겟 서버__ 의 설정이 초기화 된 경우.

```bash
node4# mkdir -p /var/lib/iscsi_disks
node4# targetcli backstores/fileio create sdb /var/lib/iscsi_disks/sdb.img 2G
node4# targetcli backstores/fileio create sdc /var/lib/iscsi_disks/sdc.img 2G
node4# targetcli backstores/fileio create sdd /var/lib/iscsi_disks/sdd.img 2G
node4# targetcli backstores/fileio create sde /var/lib/iscsi_disks/sde.img 2G
node4# targetcli iscsi/ create iqn.2023-02.com.example:blocks
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdb/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdc/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sdd/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/sde/
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node1.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node2.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node3.init
node4# targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node4.init
```




LVM2
---
1. metadata

__PV/VG/LV__ 는 전부 디스크에 특정 영역에 저장이 됨. LVM2의 기본값은 생성자(노드 혹은 호스트)의 정보를 가지고 있지 않습니다. 

생성자 정보를 가지고 있지 않으면, 어떤 노드가 LVM2 메타 정보를 사용하는지 알수가 없음. 동시에 연결을 시도하면, LVM2 메타 정보가 손상.
```bash
vgs -o+systemid 
```

메타정보는 디스크 영역에 저장.
```bash
dnf install hexedit       ## 메타정보 보여드리기 위해서
```
```bash
vi /etc/lvm/lvm.conf
> system_id_source = "none" -> uname
>> none: lvmlocal
>> uname: hostname
>> machineid: systemd(recommend in future)

vgcreate vg_ha_lvm       ## 메타정보 생성 시, 블록장치에 node1.example.com라고 기록 남김.
```

__node2/node3/node4__ 에 연결 후, 'vgs -o+systemid' 실제 디스크에 저장된 메타 정보가 아니라, 메모리에 저장된 임시 메타정보.


```bash
        # Configuration option activation/volume_list.
        # Only LVs selected by this list are activated.
        # If this list is defined, an LV is only activated if it matches an
        # entry in this list. If this list is undefined, it imposes no limits
        # on LV activation (all are allowed).
        #
        # Accepted values:
        #   vgname
        #     The VG name is matched exactly and selects all LVs in the VG.
        #   vgname/lvname
        #     The VG name and LV name are matched exactly and selects the LV.
        #   @tag
        #     Selects an LV if the specified tag matches a tag set on the LV
        #     or VG.
        #   @*
        #     Selects an LV if a tag defined on the host is also set on the LV
        #     or VG. See tags/hosttags. If any host tags exist but volume_list
        #     is not defined, a default single-entry list containing '@*'
        #     is assumed.
        auto_activation_volume_list = [ "vg1", "vg2/lvol1", "@tag1", "@*" ]

vgcreate --setautoactivation n vg_ha_lvm /dev/sdb1
```

다른 노드에서 올바르게 연결이 되지 않음. __RHEL 7 ~ 8.2__ 까지는 크게 문제 없음. 그래서 LVM2기반으로 볼륨을 공유하는 경우 가급적이면 __"--setautoactivation"__ 사용하여 자동 활성화 중지.


```bash
       --devicesfile String
              A file listing devices that LVM should use.  The file must
              exist in /etc/lvm/devices/ and is managed with the
              lvmdevices(8) command.  This overrides the lvm.conf(5)
              devices/devicesfile and devices/use_devicesfile settings.
```


NFS
---

1. 그룹이름 명시 틀림
2. 디렉터리 이름
3. LVM2의 "/home/lvm_directory"는 어디로..?
4. NFS ROOT디렉터리 무시(?)

```bash
pcs resource create nfs_share ocf:heartbeat:Filesystem device=/dev/vg_ha_lvm/lv_ha_lvm directory=/home/nfs-data fstype=xfs --future group ha_lvm_group
pcs resource create nfs_daemon ocf:heartbeat:nfsserver nfs_shared_infodir=/home/nfs-data/nfsinfo nfs_no_notify=true --future group ha_lvm_group 
pcs resource create nfs_vip ocf:heartbeat:IPaddr2 ip=192.168.90.250 cidr_netmask=24 --future group ha_lvm_group
pcs resource create nfs_notify ocf:heartbeat:nfsnotify source_host=192.168.90.250 --future group ha_lvm_group

mkdir -p /home/nfs-data/nfs-root/share01
ls -ld /home/nfs-data/nfs-root/share01

pcs resource create nfs_root ocf:heartbeat:exportfs clientspec=192.168.90.0/255.255.255.0 options=rw,sync,no_root_squash directory=/home/nfs-data/nfs-root fsid=0 --future group ha_lvm_group 

pcs resource create nfs_share01 ocf:heartbeat:exportfs clientspec=192.168.90.0/255.255.255.0 options=rw,sync,no_root_squash directory=/home/nfs-data/nfs-root/share01 fsid=1 --future group ha_lvm_group

pcs resource
pcs resource cleanup
showmount -e 192.168.90.250

node3[test]# mkdir -p /mnt/nfs-data
node3[test]# mount 192.168.90.250:/share01 /mnt/nfs-data/

df
> Filesystem              1K-blocks    Used Available Use% Mounted on
> devtmpfs                     4096       0      4096   0% /dev
> tmpfs                     1873440   48840   1824600   3% /dev/shm
> tmpfs                      749376    8748    740628   2% /run
> /dev/mapper/cs-root      73334784 2540876  70793908   4% /
> /dev/sda2                  983040  259052    723988  27% /boot
> /dev/mapper/cs-home      53895168  408860  53486308   1% /home
> /dev/sda1                  613160    7644    605516   2% /boot/efi
> tmpfs                      374688       0    374688   0% /run/user/0
> 192.168.90.250:/share01   2007040   47104   1959936   3% /mnt/nfs-data
```

NFS 연습문제
---
```bash
pvcreate /dev/sdc
vgextend vg_ha_lvm /dev/sdc
lvcreate -l 100%FREE -n lv_ha_nfs vg_ha_lvm

mkfs.xfs /dev/vg_ha_lvm/lv_ha_nfs

pcs resource create nfs_share ocf:heartbeat:Filesystem device=/dev/vg_ha_lvm/lv_ha_nfs directory=/home/nfs_directory fstype=xfs --future group nfs_group
> mount | grep /home/nfs_directory

pcs resource create nfs_daemon ocf:heartbeat:nfsserver nfs_shared_infodir=/home/nfs_directory/nfsinfo nfs_no_notify=true --future group nfs_group
> ls -l /home/nfs_directory/nfsinfo

pcs resource create nfs_vip ocf:heartbeat:IPaddr2 ip=192.168.90.250 cidr_netmask=24 --future group nfs_group
pcs resource create nfs_notify ocf:heartbeat:nfsnotify source_host=192.168.90.250 --future group nfs_group
> ip a s eth1

mkdir -p /home/nfs_directory/nfs_data
> ls -ld /home/nfs_directory/nfs_data

pcs resource create nfs_root ocf:heartbeat:exportfs clientspec=192.168.90.0/255.255.255.0 options=rw,sync,no_root_squash directory=/home/nfs_directory/nfs_data fsid=0 --future group ha_lvm_group 
> rpm -qi nfs-utils 
> showmount -e <IP_ADDR>
> journalctl -perr -pwarning _COMM
> journalctl -perr -pwarning -u <UNIT.SERVICE>

node3[test]# showmount -e 192.168.90.250
node3[test]# mkdir -p /mnt/nfs-data
node3[test]# mount 192.168.90.250:/share01 /mnt/nfs-data/
```

APACHE 연습문제
---

```bash
pvcreate /dev/sdd
vgextend vg_ha_lvm /dev/sdd
lvcreate -l 100%FREE -n lv_ha_httpd vg_ha_lvm

mkfs.xfs /dev/vg_ha_lvm/lv_ha_httpd
> lvdisplay
> vgs -o+systemid

for i in {1..4} ; do ssh root@node${i} "dnf install httpd -y" ; done
cat <<EOF> /etc/httpd/conf.d/server-status.conf
<Location /server-status>
    SetHandler server-status
    Require local
</Location>
EOF
> systemctl start httpd
> curl 127.0.0.1/server-status 
> systemctl stop httpd

for i in {1..4} ; do scp /etc/httpd/conf.d/server-status.conf root@node${i}:/etc/httpd/conf.d/ ; done

mount /dev/vg_ha_lvm/lv_ha_httpd /var/www/html
echo "Hello Apache" > /var/www/html/index.html
umount /var/www/html
> systemctl start httpd
> curl 192.168.90.110
> systemctl stop httpd

pcs resource create httpd_fs ocf:heartbeat:Filesystem device=/dev/vg_ha_lvm/lv_ha_httpd directory=/var/www/html fstype=xfs --future group httpd_group
> pcs resource

pcs resource create httpd_vip ocf:heartbeat:IPaddr2 ip=192.168.90.254 cidr_netmask=24 nic=eth1 --future group httpd_group
> pcs resource
> nic=<INTERFACE_NAME>

pcs resource create website ocf:heartbeat:apache configfile=/etc/httpd/conf/httpd.conf statusurl=http://127.0.0.1/server-status --future group httpd_group
> pcs resource

curl http://192.168.90.254/index.html

```