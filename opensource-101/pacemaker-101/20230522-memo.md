# DAY 1


## 랩 구성

```bash
              +---------------------+
              | switch for external | - Default Switch(hyper-v)
              +---------------------+ - NAT(VMware)                    
                         |
                         |
                        [external]
                        NIC(vmnet0)
                         | (eth0,hyper-v)
                         |
                    +--------+
                    |   VM   |  
                    +--------+
                         |
                         | (eth1,hyper-v)
                        NIC(vmnet1)
                        [internal]   VM to VM
                         |
                         |
              +---------------------+
              | switch for internal | - internal(hyper-v)
              +---------------------+ - host(VMware)                     
```

[윈도우 터미널 설치](https://learn.microsoft.com/ko-kr/windows/terminal/install)

[하이퍼브이 설치 메뉴얼](https://learn.microsoft.com/ko-kr/virtualization/hyper-v-on-windows/quick-start/enable-hyper-v)

- 네트워크
    + internal: 내부 네트워크(하이퍼브이 및 다른 하이퍼바이저)
    + external: 외부 네트워크(하이퍼브이는 'default')
    + storage: 저장소 네트워크(없어도 상관은 없음)
- 가상머신
    + 최소 가상머신 3대로 구성
    + 가상머신의 이름은 node1/2/3으로 구성
    + vMEM: 4096MB(최소2048)
    + vCPU: 2개 권장
    + vDISK: Thin, 100GB/Fixed, 30GB
- 리눅스 이미지
    + CentOS-9-Stream(강의에서는 이 이미지 기반으로 진행)
    + Rocky-9(RHEL Clone)
    + Oracle-Linux-9
    + OpenSuSE
    + Ubuntu/Debian
    

### 네트워크 설정


두 번째 네트워크 카드의 주소를 고정 아이피로 설정.

- nmcli
- nmtui

```bash
nmcli con show
NAME  UUID                                  TYPE      DEVICE
eth0  0c510583-c37d-398f-835d-2c593f009085  ethernet  eth0
eth1  552198fe-f5a3-31d7-a945-2394b577c620  ethernet  eth1
lo    5426e314-719c-408d-812b-a1be8b0b6146  loopback  lo

nmcli dev

nmcli con mod
          add    ifname ens160

nmcli con down eth1
nmcli con up eth1
nmcli con sh eth1 | grep ipv4. | less

nmcli del <UUID>                            ## 필요없는 프로파일 제거

```
"프로파일"이름이 장치이름으로 안되어 있는 경우 'add'로 프로파일 추가 생성.

아이피 주소는 다음과 같이 구성

| 노드 이름 | 아이피 주소 | 게이트웨이|
|-----------|-------------|-----------|
| node1     | 192.168.90.110/24 | N/A |
| node2     | 192.168.90.120/24 | N/A |
| node3     | 192.168.90.130/24 | N/A |
| node4     | 192.168.90.140/24 | N/A | 

**혹시나 하여, eth1를 기본 라우트 아이피로 사용하지 않음.

하이퍼브이
---

```bash

## node1 
nmcli con mod eth1 ipv4.addresses 192.168.90.110/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node2
nmcli con mod eth1 ipv4.addresses 192.168.90.120/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node3
nmcli con mod eth1 ipv4.addresses 192.168.90.130/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1

## node4 
nmcli con mod eth1 ipv4.addresses 192.168.90.140/24 ipv4.never-default yes ipv4.method manual autoconnect yes
nmcli con up eth1
```


VMware, VirtualBox, Libvirt
---

장치이름이 eno1, ens1...이름이 다르게 구성이 됨.

- 프로파일 이름은 보통 장치 이름과 동일하게 구성
- 변경해야 될 부분은 'con-name', 'ifname', 'type'
- con-name, connection name의 줄임
- ifname, interface card name의 줄임

```bash
## node1
nmcli con add con-name ens1 ipv4.addresses 192.168.90.110/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node2
nmcli con add con-name ens1 ipv4.addresses 192.168.90.120/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node3
nmcli con add con-name ens1 ipv4.addresses 192.168.90.130/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

## node4
nmcli con add con-name ens1 ipv4.addresses 192.168.90.140/24 ipv4.never-default yes ipv4.method manual autoconnect yes type ethernet ifname ens1 
nmcli con up ens1

```

## 페이스 메이커 소개


페이스메이커는 스스로 H/A기능을 제공하지 않음. H/A를 구성하기 위해서 사용하는 자동화 도구. 


[에이전트 개발자 가이드](https://github.com/ClusterLabs/resource-agents/blob/main/doc/dev-guides/ra-dev-guide.asc)


### 9 이론

90~99%: 애플리케이션 영역, 90% H/A애플리케이션이 구성이 되었을 때. 

- 99%: 일단위로 전환
- 99.9%: 시간단위로 전환
- 99.99995: 10초 이내

보통 보고서에 많이 적는 문구 "99.999%" 복구율 보장(2~5분 사이). 페이스메이커는 99.999% ~ 99.9999%를 지원.

99.99%를 지원하는 경우, 페이스메이커가 멀티 사이트(Multi-Site)로 구성이 되어 있는 경우.

본레 페이스메이커는 D/R를 지원하지 않음. RHEL 8버전 이후부터는 D/R 및 BOOTH기능이 추가가 됨.

D/R, H/A 리소스를 그닥 효율적으로 사용하지 않다. 
- A/A가 아닌 경우 
- 컨테이너 시스템에서 D/R, H/A가 필요하지는 않음


## 설치 준비

```bash

              |     pacemaker    |   <-- systemctl start pcsd
              +------------------+
                \
                 `---> crm_*
```





```bash
dnf grouplist 

## 확장 패키지는 앞으로는 모듈로 제공(app-stream)
dnf module list

## /etc/yum.repos.d/centos-addons.repo에서 사용하지 않도록 설정이 되어 있음. 
ls -l /etc/yum.repos.d/centos-addons.repo
vi /etc/yum.repos.d/centos-addons.repo
enabled=0 --> 1

## 옵션으로 저장소 활성화
dnf --enablerepo=highavailability search pacemaker pcs
dnf --enablerepo=highavailability install pacemaker pcs -y     ## 모든 노드에 설치

systemctl is-active pcsd.service
systemctl enable --now pcsd.service

locale 
> ko_KR.utf8 --> en_US.utf8
             --> C
localectl set-locale en_US.utf8   ## 1
export LC_ALL=en_US.utf8          ## 2

kill -9 1754
dnf install glibc-langpack-en -y

localectl set-locale C
export LC_ALL=C


hostnamectl                                                ## 호스트 이름 확인
cat <<EOF>> /etc/hosts                                     ## node3번에서 작업중 
192.168.90.110 node1.example.com node1                     ## sda,b,c,d(iscsi)
192.168.90.120 node2.example.com node2                     ## sda,b,c,d(iscsi)
192.168.90.130 node3.example.com node3                     ## 블록장치를 추가 b,c,d + e,f,g(iscsi)
192.168.90.140 node4.example.com node4 storage cli utility ## 파일기반으로 블록장치 구성
EOF

sshpass -pcentos scp /etc/hosts node1:/etc/hosts

ping -c2 node{1..3}

ssh-keygen -t rsa -N'' ~/.ssh/id_rsa                      ## ssh 비공개/공개키 생성
dnf install sshpass -y                                    ## sshpass 패스워드 입력 대신  
cat <<EOF> ~/.ssh/config                                  ## fingerprint 무시
StrictHostKeyChecking=no
EOF
sshpass -pcentos ssh-copy-id root@node{1..3}              ## node1~3번까지 공개키 전달
sshpass -pcentos scp /etc/hosts node{1..3}:/etc/hosts

## 루트 로그인이 안되시는 경우
## 현재 기본값 PermitRootLogin prohibit-password
## RHEL9이나 혹은 rocky 9, Centos-9-Stream에서 변경사항

/etc/ssh/sshd_config.d/01-rootallow.conf
PermitRootLogin yes                                       ## /etc/ssh/sshd_config의 값 오버라이드 

## SSH passphase키 질문 부분
vi /etc/ssh/sshd_config                                   ## 앞으로는 절대로 직접 수정하지 말것
cat <<EOF> /etc/ssh/sshd_config.d/02-keyallow.conf
PubkeyAuthentication yes
EOF

systemctl restart sshd

## node 1~3번까지 확인

sshpass -pcentos ssh node1 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node2 "hostname && grep hacluster /etc/passwd"
sshpass -pcentos ssh node3 "hostname && grep hacluster /etc/passwd"


grep hacluster /etc/passwd                            ## pacemaker hacluster 사용자, PCS 루트 사용자

## node1~3번까지 패스워드 설정

echo cluster | passwd --stdin hacluster
grep hacluster /etc/passwd
hacluster:x:189:189:cluster user:/home/hacluster:/sbin/nologin               ## pcs명령어 접근 및 실행시 사용

systemctl enable --now pcsd.service                                          ## node1, node2, node3
systemctl stop firewalld                                                     ## 방화벽 중지
pcs host auth -u hacluster -p cluster node1.example.com node2.example.com  node3.example.com  ## node3
> node1.example.com: Authorized
> node2.example.com: Authorized
> node3.example.com: Authorized
#
# /var/lib/pacemaker 각 노드끼리 토큰 인증
#

pcs status
pcs cluster status
pcs cluster setup ha_cluster_lab node1.example.com node2.example.com node3.example.com
                  -------------  ----------------- -----------------
                  클러스터 이름      노드 이름         노드 이름       .......
--> 만약에 이전 내용이 존재하면 제거를 시도
--> pcs == pcsd == pacemaker
--> corosync == quorum
pcs cluster start --all
pcs cluster enable --all 

pcs cluster status
pcs status

## pcs ---> pcsd.service


ls -laR /etc/corosync
ls -laR /etc/pacemaker
ls -laR /var/lib/corosync
ls -laR /var/lib/pacemaker
``` 



페이스메이커 설치시 가급적이면, terraform, ansible, puppet를 통해서 자동으로 설치 및 구성을 권장.
- 레드햇 계열은 앤서블 기반으로 설치



### 컨테이너 잠깐...

docker ---> podman 
                    ---> kubernetes ---> pod ---> svc


```bash
dnf install epel-release -y
dnf install podman podman-compose podman-docker podman -y
dnf install git -y

git clone https://github.com/docker/awesome-compose/
> tree/master/apache-php
podman-compose up -d
podman container ls
podman kube generate apache-php_web_1 --service -f apache.yaml 

kubectl apply -f apache.yaml

```

### 레드햇 클론의 고뇌?

```bash
                  ---- 간주 넘기기 ----> 
 과도기                  안정버전((레거시))   컨테이너+가상화+OS REV
                  |
 RHEL 7           |       RHEL 8            |        RHEL 9             |   RHEL  X(10)
                  |
+ systemd           + systemd                + systemd(완성형)
+ NetworkManager    + xfs rev.2              + xfs rev(d)
+ teamd             + kernel --> 4.x         + kernel 5.x 
+ OCI               * container              + container 성능
+ xfs rev.1                                  + network, block 성능 
                                             + iptables -> nftable(firewalld)
                                             + NetworkManager(d)
                                             + LVM2 --> Stratis, Vdo(R)
                                             
```

RHEL 9에서 많이 달라진 부분
---

1. 커널: 컨테이너+네트워크+프로세스
2. 블록장치: xfs, SGI 1:1, 성능위주 ---> 편의성+
3. 네트워크 TCP Stack, 다중처리에 초점(컨테이너 및 가상머신), eBPF(현 리눅스, cBPF)
4. 스토리지: Stratis Pool, Vdo 

__컨테이너 사용하시는 경우__

가급적이면, SELinux나 혹은 AppAmmor [Selinux vs AppArmor](https://phoenixnap.com/kb/apparmor-vs-selinux)

리눅스 파운데이션   --> AppArmor      ## 커뮤니티...장점이 상대적으로 사용하기 편하다.
엔터프라이즈 리눅스 --> SELinux       ## NIST 표준임


아직 까지는 LVM2가 리눅스에서 Pool기준임. 
- btrfs, 네이티브 스토리지 풀링, zfs, ufs, jfs와 동일한 기능
- Stratis(LVM2를 재구성), xfs전용


# DAY 2


## 정리

프로비저닝
---
- satellite == foreman(dhcp,tftp,pxe)
- dhcp, tftp, pxe


디폴로이먼트
---
- ansible(레드햇 권장)
- terraform
- SH script(현재는 권장하지 않음)
- 내부 NTP서버도 필요(방화벽에서 특정 서버만 허용)

1. 페이스메이커 설치는 폐쇠망에서 설치
2. RPM에 대한 저장소 미러링이 필요함
3. 자동화 도구(예,앤서블) 기반으로 설치 및 설정을 자동화
4. NTP서버
5. Corosync(시간에 매우 민감)



### iscsi 서버 구성

1. 실제로 물리적인 스토리지 서버 사용이 불가능
2. 자원이 넉넉하지 않는 관계로, 파일 기반의 블록장치를 만들어서 배포
3. target기반으로 각각 서버에서 /dev/sdb,c,d를 배포

```bash
dnf install targetcli -y                                ## node4번에만 설치
systemctl enable --now target
firewall-cmd --add-service=iscsi-target                 ## 방화벽 사용을 안하시면 해당 부분 무시
dnf install iscsi-initiator-utils -y                    ## 모든노드에서 설치가 되어야 됨
```

```bash

## fileio 파일 기반으로 가상의 디스크를 생성
## 여기서 사용하는 이름은 분류를 위한 파일 네이밍

setenforce 0
getenforce
mkdir -p /var/lib/iscsi_disks/
targetcli backstores/fileio create iscsi /var/lib/iscsi_disks/iscsi_disk.img 2G
targetcli backstores/fileio create nfs /var/lib/iscsi_disks/nfs_disk.img 2G
targetcli backstores/fileio create gfs2 /var/lib/iscsi_disks/gfs2_disk.img 2G
ls -l /var/lib/iscsi_disks/
file /var/lib/iscsi_disks/iscsi_disk.img
> /var/lib/iscsi_disks/iscsi_disk.img: data
targetcli ls

targetcli iscsi/ create iqn.2023-02.com.example:blocks  
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/iscsi/
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/nfs/
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/luns/ create /backstores/fileio/gfs2/
targetcli ls


## IQN ACL 구성 

targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node1.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node2.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node3.init
targetcli iscsi/iqn.2023-02.com.example:blocks/tpg1/acls/ create iqn.2023-02.com.example:node4.init
targetcli ls

firewall-cmd --add-service=iscsi-target
firewall-cmd --runtime-to-permanent

systemctl stop firewalld                                      ## 방화벽 필요 없으면

targetcli saveconfig

## 각 노드에 다음과 같이 수정

vi /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2023-02.com.example:nodeX.init

cat <<EOF>> /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2023-02.com.example:nodeX.init
EOF

## 각 노드에서 아래와 같이 명령어 실행
systemctl restart iscsi iscsid
iscsiadm -m discovery -t sendtargets -p 192.168.90.140 
> 192.168.90.140:3260,1 iqn.2023-02.com.example:blocks              ## 올바르게 iscsi 질의 완료

iscsiadm -m node --login                                            ## 각 노드에서 실행
lsblk 
> sdb           8:16   0     2G  0 disk
> sdc           8:32   0     2G  0 disk
> sdd           8:48   0     2G  0 disk
```

## 클러스터 상태 확인

RHEL 7 이후에 pacemaker는 /etc/에 있는 설정 파일을 수동으로 편집을 하는 경우가 없음. RHEL 8/9부터는 수동으로 편집을 권장하지 않음. 

일반적인 장애들은, SELinux나 혹은 /var/에서 디렉터리 손실이나 혹은 설정(퍼미션 같은) 문제로 발생.

```bash
pcs status                                  ## 페이스메이커 설정 확인
pcs cluster status                          ## pcsd, corosync 서비스 상태
pcs status corosync                         ## corosync의 구성원 현황

systemctl status pcsd pacemaker corosync
          is-active

# man corosync                                ## /etc/corosync/

ls -l /etc/corosync/ 
> corosync.conf                             ## corosync 설정파일. pcs명령어에 따라서 내용이 변경
ls -l /etc/pacemaker/
> authkey                                   ## pcs auth로 통해서 인증 받은 정보가 있음. 페이스메이커 관리자 계정
ls -l /etc/ha.d/                            ## OCF 스크립트 펑션
ls -l /var/pacemaker/ 
                     cib/cib.xml            ## corosync에서 사용하는 노드 동기화 정보           
ls -l /var/pcsd/
                known-hosts                 ## ss -antp | grep 2224
ls -l /var/corosync/
```

로그 부분

```bash
/var/log/pcsd                               ## pcsd 로그
         pacemaker                          ## pacemaker 로그
         cluster                            ## corosync 로그


journalctl                                  ## /run/log/journal

## 영구적으로 기록을 남기게 하려면 아래와 같이 (쉬운 방법)
cp -a /run/log/journal/   /var/log/journal
systemctl daemon-reload
systemctl restart systemd-journald

## pcsd pacemaker corosync

journalctl -u pcsd                                        ## /var/log/pcsd
journalctl -u pacemaker -fl -p err -p warning -p info     ## /var/log/pacemaker/ 
journalctl -u corosync                                    ## /var/log/cluster/
journalctl -u pacemaker -p err -p warning -o cat          ## -o, --output 형태로 파일 출력
```

### 쿼럼 명령어

가급적이면 동기화 명령어는 사용하지 말것.

```bash
corosync-quorumtool -s 
corosync-cfgtools -s
```

연습문제
---
**잠깐 문제(5분)**

journalctl명령어를 사용해서 pcsd, pacemaker, corosync에서 발생한 오류 및 경고 메세지 확인.
발생한 오류 메세지를 텍스트 형태로 각각 서비스 이름으로 .txt확장로 저장.



### 간단한 자원 구성


```bash

# wwn-0x6001405ad580a9178654d349502eed83
# wwn-0x6001405ad580a9178654d349502eed83

ls -l /dev/disk/by-id/wwn-*   --> /dev/sdb

pcs stonith create scsi-shooter fence_scsi pcmk_host_list="node1.example.com node2.example.com" devices=/dev/disk/by-id/wwn-<SDB> meta provides=unfencing 
pcs stonith 
pcs stonith config scsi-shooter 
pcs stonith fence node2.example.com

node2> reboot                                          ## 윈도우 혹은 맥에서 하시는 경우 수동으로 리붓. VBMC자동으로 가상머신 리부팅
```


### 질답시간

1. IPMI로 팬싱시켜서 리붓시키는건 어느정도 이해가 되는데 scsi fencing은 어떤 매커니즘으로 노드 차단이 되는건지 순서대로 간략하게 알 수 있을까요? scsi 팬싱이 어떤 방식으로 동작되는건지 궁금해서요! 

펜싱 방법

- 에이전트에 특정 자원 위치 정보가 등록
  + 모니터링 할 대상 노드도 같이 들어감
  + pcmk_host_list==pacemaker_host_list="node1.example.com, node2.example.com"
  + devices="/dev/sdb"
- 특정 장치 모니터링 시작
  + 장치가 존재 유무 확인
  + 없는 경우 특정 횟수동안 계속 확인
- 문제가 확실해지면 특정 노드 리부팅
  + node2.example.com, reboot
- 장치가 올바르게 다시 인식이 되면 정상
- 장치가 올바르게 인식이 다시 되지 않으면 노드에서 제외

```bash
pcs stonith list                                  ## 보통은 파이선으로 작성이 되어 있음
rpm -ql fence-agents-scsi
> /usr/sbin/fence_scsi                            ## file /usr/sbin/fence_scsi

                               .---> 특정 횟수동안 응답이 없으면 장애 판명
                              /
                  -----------'
pcs stonith create fence_scsi  devices=/dev/disk/by-id/wwn-<SDB> meta provides=unfencing 
                                       -------------------------
                                    에이전트가 블록장치를 모니터링
pcs stonith fence node2.example.com               ## 1. SCSI DEVICE 차단, 2. 재시작(ipmi)
            -----
            \
             '---> 강제로 클러스터에 차단
```

2. pcs stonith create scsi-shooter fence_scsi pcmk_host_list="node1.example.com node2.example.com" devices=/dev/disk/by-id/wwn-<SDB> meta provides=unfencing 

stonith: Shoot the other node in the head 

일반적으로 노드 펜싱(fencing)실행하면 ipmi를 통해서 재시작. 


3. 리부팅후 pcs status 하면 Full List of Resources: * scsi-shooter        (stonith:fence_scsi):    Stopped

상태가 "stopped"인 경우는, 자원 구성이 안되어 있어서...

- resource 목록에 있는 자원을 가지고 HA자원 구성
- stonith 목록에 있는 에이전트를 가지고 차단을 구성 


### 랩 시작전


명령어 좀 더 쉽게 사용하기

```bash
dnf search bash
> bash-completion
dnf install bash-completion -y
complete -r -p
exit
ssh root@nodeX
```


## booth

Booth "Csync"기반으로 동작


## two-node with qdevice

https://access.redhat.com/solutions/1294873

RHEL 8버전부터는 TWO상태를 corosync가 인식 및 인지를 함. 
qdevice, 

```bash

vi /etc/corosync/corosync.conf
## wait_for_all is enabled implicitly because two_node is enabled
quorum {
    provider: corosync_votequorum
    two_node: 1
}

# # wait_for_all is enabled explicitly
quorum {
    provider: corosync_votequorum
    two_node: 1
    wait_for_all: 1
}

# # wait_for_all is disabled because two_node is not enabled
# # wait_for_all is not set explicitly so it defaults to 0 here
quorum {
    provider: corosync_votequorum
}

# # wait_for_all is disabled explicitly
quorum {
    provider: corosync_votequorum
    two_node: 1
    wait_for_all: 0
}

```

### 스토리지 구성

#### 기본 블록 장치

ext3,4/xfs같은 파일 시스템은 단일 저널링만 제공함. 
GFS2를 통해 다중 저널링 제공 및 공유
xfs/ext다중 연결(read write)파일 시스템 S/B(슈퍼블럭)이 깨짐
btrfs파일 시스템 제외하고 나머지 대다수 리눅스 파일 시스템은 pool기능 및 shadow block copy를 지원하지 않음
- stratis(xfs pool)

Node1

```bash
mkdir -p /mnt/sdb
mkfs.xfs /dev/sdb
mount /dev/sdb /mnt/sdb
ls -l *
```

Node2

```bash
mkdir -p /mnt/sdb
mount /dev/sdb /mnt/sdb
cp /mnt/sdb/node1.txt /mnt/sdb/node2.txt
ls -l *
```


LVM2

```bash
umount /dev/sdb                           ## node1, node2 디스크 연결해제
wipefs -a /dev/sdb
cfdisk /dev/sdb                           ## 마음대로 LVM2
> 파티션 1 LVM2
dnf install hexedit -y
pvcreate /dev/sdb1
vgcreate testvg /dev/sdb1

vgremove /dev/sdb1 testvg
pvremove /dev/sdb1
wipefs -a /dev/sdb


parted --script /dev/sdb "mklabel msdos"
parted --script /dev/sdb "mkpart primary 0% 100%"
parted --script /dev/sdb "set 1 lvm on"

pvcreate /dev/sdb1
vgcreate vg_ha_lvm /dev/sdb1
vgs -o+systemid
lvcreate -l 100%FREE -n lv_ha_lvm vg_ha_lvm

mkfs.xfs /dev/vg_ha_lvm/lv_ha_lvm

## vgcreate 할때 --setautoactivation n
vgchange vg_ha_lvm -an
lvm pvscan --cache --activate ay

## pcs resource list

pcs resource create lvm_ha_iscsi ocf:heartbeat:LVM-activate vg_name=vg_ha_lvm vg_access_mode=system_id --group ha_lvm_group
pcs resource create lvm_ha_mount FileSystem device=/dev/vg_ha_lmv/lv_ha_lvm directory=/home/lvm_directory fstype=xfs --group ha_lvm_group
```

# day 3


## 랩 및 질문답변

__kvm에서 테스트 할때 poweroff했는데 그냥 종료가 되는 현상__
> vbmc이슈.. RHEL8까지는 크게 문제 없다가, RHEL 9에서는 오동작이 좀 있음. 
> host 8.7, libvirtd 8.0.0, vbmc 2.2.2

__왜 노드1번의 LVM이 안넘어 가는지?__
> 8까지는 크게 문제 없음. 9부터 "system_id"가 계속 걸림. 
> 첫번째 강사의 랩 실수(system_id_source = "uname")
> 여기에 대한 해결책 솔루션(진행중)
> node1# umount
> node1# vgchange -an vg_ha_lvm 
> pvs --foreign
> lvmdevices --adddev wwn-0x6001405ad580a9178654d349502eed83-part1
> vgs --foreign vg_ha_lvm
> vgs -ao+systemid
> vgchange -ay vg_ha_lvm --config 'activation { volume_list = [ "@vg_ha_lvm" ]}'
> vgchange -y --config local/extra_system_ids=[\"node1.example.com\"] --systemid node2.example.com >>vg_ha_lvm

__install__
> https://www.geeksforgeeks.org/install-command-in-linux-with-examples/

__pcs node utilization node1__
> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-node-placement-strategy-configuring-and-managing-high-availability-clusters

__dc(designated coordinator,지정 코디네이터)__
> https://access.redhat.com/articles/2067543#pacemaker_daemons

__dr 구성할 대 Site1(node1,node2)가 모두 죽으면 Site2(node3,node4)가 서비스 시작 되는 개념인가요?__
> https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-disaster-recovery-configuring-and-managing-high-availability-clusters
>  As of Red Hat Enterprise Linux 8.2, you can use the pcs command to display the status of both the primary and the disaster recovery site cluster from a single node on either site. 

pacemaker는 heartbeat 별도로 구성 안해줘도 되나요?
> https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Explained/singlehtml/


```
fdisk: mbr(dos)  ---.
                     \
                      > cfdisk(gpt, mbr)
                     /
gdisk: GPT(uefi) ---'


Fedora --> CentOS --> RHEL 
```

pvcreate: 저널링 혹은 블록 장치에 직접 구성할 내용이 있는 경우, 옵션으로 조정이 가능.

vgcreate: VG통해서 LV나 혹은 VG를 다른 노드에서 접근 및 조회. 현재, 페이스메이커에서 VG를 설정을 민감하게 사용하고 적용중.
- vgcreate --shared --locktype dlm --systemid <NAME>
  + "--systemid"가 다르면 다른노드에서 정책위반으로 판단으로 차단
- LVM2가 RHLE 7까지는 AIX LVM에서 기능만 흉내는 정도?
  + RHEL 8부터 LVM2를 강화하기 시작함
  + RHEL 8에서 사용하는 LVM명령어들은 실제 AIX LVM하고 거의 비슷하게 동작
  + RHEL 9에서는 완전히 정책을 강하게 적용
  + vgchange -ay vg_ha_lvm --config와 같이 설정 변경 요구

```bash
mandb

```


# DAY 4

RHEL 9
LVM에서 meta정보가 다른 경우, LVM scan시 무시.



RHEL 7/8까지는, LVM기반으로 구성한 xfs경우, 크게 문제 없이 다른 노드에서 연결이 가능


- 파일 시스템
  + XFS, 단일 저널링 파일 시스템
  + PV,VG,LV공유 가능한 자원
  + lvm meta 공유(shared)
  + 여러 노드에서 접근이 불가능함
  + BTRFS가 현재 다중 파일 시스템 연결을 유일하게 지원
  
- 웹 서버 혹은 데이터베이스
  + 레이턴시 부분(지연)에 민감한 경우 방법을 다르게 생각해야됨
    * cephfs, btrfs, native san storage
  + 소프트웨어가 이중화 기능을 제공하지 않는 경우
  + H/A최대 단점이라고 하면, 애플리케이션 가동율에 대해서 보장이 어려움
  + 낮은 레이턴시를 요구하는 시스템에서는 저장소는 고려사항
  
- 공유 파일 시스템
  + GFS2
  + Ceph
  + Lustre
  + Glusterfs
  + btrfs
  + 어떤걸 사용할것인가?
  


## LVM2 관련 내용

1. locking_type
2. use_lvmlockd
3. system_id_source


```bash
pcs resource list | grep lvm
man -k ocf_heartbeat_lvmlockd
> locking_type = 1
> use_lvmlockd = 1    -->    systemctl  status lvmlockd
                             dnf install lvm2-lockd 

lvmconfig                               ## 모든 서버에서 /etc/lvm.conf
>global {                      
>  locking_type = 1
>  use_lvmlockd = 1
>}
```
LVM락킹은 정말 구려요!! :(
---

특정 VG만 lvmlocked관리가 필요한 경우.

```text
       activation {
           lock_start_list = [ "vg1" ]
           ...
       }
```

__vgcreate__
  - ~~clustered, 이전에 clvmd에서 사용한 옵션. 지금은 lvmlockd로 변경, 사용안해도 됨.~~
  - shared, 여러 호스트에서 VG를 공유하는것을 허용
    + man 8 lvmlockd
  - locktype, dlm으로 사용 권장. san 스위치는 sanlock으로 권장.
  
lvcreate
  - 
vgs -o+systemid
>호스트 이름으로 메타 정보에 기록
> cs   1   3   0 wz--n- 125.41g    0

lvcreate -vvv lv_nfs -l 100%Free vg_nfs
>setting global/locking_type to 1

```bash
vgchange --config 'global/locking_type=3' --lock-type dlm vg1
dlm_tool ls 
```


[수세 lvm.conf locking_type 문서](https://www.suse.com/support/kb/doc/?id=000018046)
[레드햇 LVM문서](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/pdf/configuring_and_managing_logical_volumes/red_hat_enterprise_linux-8-configuring_and_managing_logical_volumes-en-us.pdf)

[락킹1](https://sourceware.org/git/?p=lvm2.git;a=commitdiff;h=f611b68f3c02b9af2521d7ea61061af3709fe87c)
[락킹2](https://lore.kernel.org/all/f77fe769-5bb9-58c4-89ca-7a707a78deac@suse.com/T/)
