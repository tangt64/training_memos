# day 1

__이름:__ 최국현<br/>
__이메일__: tang@linux.com<br/>


메모주소
---
https://github.com/tangt64/training_memos/
>redhat-offical-training
>/RH436/20231017-memo.md

https://github.com/tangt64/training_memos/
>/opensource-101
>/pacemaker-101/

랩 및 사이트 접속 주소
---
https://rol.redhat.com

__"리소스"__ --> __"e-book다운로드"__ 가능 합니다.

가입하신 __아이디/비번__ 으로 접근하시면 됩니다.


pcs명령어 자동완성이 안되면

```bash
completion -r -p
source /etc/bashrc

ls -l /etc/bash_completion.d/
ls -l /usr/share/bash-completion/completions/
```


```bash
>getenfroce 
>setenforce 0

dnf install pcs fence-agents-all -y 
echo redhat | passwd --stdin hacluster 
systemctl enable --now pcsd 

firewall-cmd --get-services
firewall-cmd --add-service high-availabilty --permanent
firewall-cmd --reload(opt1)

firewall-cmd --add-service high-availabilty
firewall-cmd --runtime-to-permanent (opt2)

>systemctl stop firewall 

pcs host auth -u hacluster -p redhat node1 node2 node3
pcs cluster create --enable --start node1 node2 node3
pcs status
```

레드햇 참고 링크
---
[레드햇 솔루션 two_node](https://access.redhat.com/solutions/1294873)
[레드햇 솔루션 two_node옵션 설명](https://access.redhat.com/solutions/1293543)

```bash
pcs cluster stop --all
vi /etc/corosync/corosync.conf
> two_node: 1
> wait_for_all: 1
> last_man_standing: 1
> auto_tie_breaker: 1
> auto_tie_breaker_node: 1 3 5
>> auto_tie_breaker_node: lowest
>> auto_tie_breaker_node: highest
pcs cluster sync
pcs clustart start --all
```

# day 2

pacemaker: (p)a(c)e(m)a(k)er

STONITH
---
```
  +-----------+
  | pacemaker |
  +-----------+
    |       |                 .---> resource
    |       |                /
  pcsd  pacemaker --> agent <
    |       [pcmk]           \
    |                         `---> stonith
    v
     # pcs sync(corosync)
       agent
```

IPMI(drac,ILO)
---
```
     VirtualBMC <--> OpenBMC
         |              |
         |            vender
         |
       IPMI(poweroff, reboot)
       /  \
      /    \
    drac   ILO
```



```bash
/var/lib/pacemaker/cib

cibadmin --query --local
cibadmin --replace --xml-file <XML_FILE>

crm_standby -N nodea.private.example.com
crm_resource --list-agents ocf
             -N nodea.private.example.com
crm_report -f "2023-10-17 00:00:00" /tmp/report
           -f "2023-10-17 00:00:00" -n nodec /tmp/report-nodec
           -f "2023-10-17 00:00:00" --sos-mode /tmp/report-nodec-sosreport
crm_error -l

crm_simulate
```

[리소스 이동](https://clusterlabs.org/pacemaker/doc/deprecated/en-US/Pacemaker/1.1/html/Clusters_from_Scratch/_move_resources_manually.html)

[페이스메이커 스케줄러](ttps://github.com/ClusterLabs/pacemaker/blob/main/lib/pacemaker/pcmk_scheduler.c)


# day 3

Pacemaker Scheduler
---
https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Development/html/components.html#scheduler

[페이스 메이커 설명](https://clusterlabs.org/pacemaker/doc/2.1/Pacemaker_Explained/singlehtml/)


Node/Resource
---

리소스 대상
1. OCF
2. LSB
3. Systemd
4. Service
5. Fencing

리소스에 들어가는 옵션은 "resource-meta"라고 부름.

| 옵션                 | 적용            | 설명 |
|----------------------|----------------|------|
| resource-stickiness | 1 for individual clone instances, 0 for all other resources | __A score that will be added to the current node when a resource is already active__. This allows running resources to stay where they are, even if they would be placed elsewhere if they were being started from a stopped state. |

1. 노드에 자원이 활성화/할당이 된 경우
2. 자원이 동작할 노드 위치

* Set a higher value for resource-stickiness during working hours, to minimize downtime, and a lower value on weekends, to allow resources to move to their most preferred locations when people aren’t around to notice.
* Automatically place the cluster into maintenance mode during a scheduled maintenance window.
* Assign certain nodes and resources to a particular department via custom node attributes and meta-attributes, and add a single location constraint that restricts the department’s resources to run only on those nodes.

| 옵션                 | 적용            | 설명 |
|----------------------|----------------|------|
| score,score-attribute|                |If this rule is used in a location constraint and evaluates to true, apply this score to the constraint. Only one of score and score-attribute may be used. |


>When using score-attribute instead of score, each node matched by the rule has its score adjusted differently, according to its value for the named node attribute. Thus, in the previous example, if a rule inside a location constraint for a resource used score-attribute="cpu_mips", c001n01 would have its preference to run the resource increased by 1234 whereas c001n02 would have its preference increased by 5678.

[Penguin](https://github.com/ClusterLabs/pacemaker/blob/835e61470ed2b0e462d967e7d37beaec71fca6a9/lib/pengine/common.c)
---
pacemaker에서 사용하는 리소스의 프로세스 상태 확인.


blackbox
---
pacemaker, corosync에서 발생하는 로그 관리.

```bash
ls -l /dev/shm/qb-corosync-*
```

[CPG](https://access.redhat.com/documentation/ko-kr/red_hat_enterprise_linux/6/html/6.3_release_notes/clustering-high_availability)
---
Closed Process Group, luci(RHEL 6)에서 사용하던 기능. 


CIB활용?
---

```bash
cibadmin --query --local > /root/backup.xml
cp /etc/corosync/corosync.conf /etc/corosync/corosync.conf.back

pcs cluster stop --all
pcs cluster disable --all
pcs cluster destroy

pcs host auth -u hacluster -p redhat nodea.private.example.com nodeb.private.example.com nodec.private.example.com 
pcs cluster setup replica-cluster --enable --start --wait nodea.private.example.com nodeb.private.example.com nodec.private.example.com

pcs status

cibadmin --replace --xml-file  /root/backup.xml

```


```bash
cibadmin --query --local > /root/backup.xml
cibadmin --replace --xml-file  /root/backup.xml
```

https://www.thomas-krenn.com/en/wiki/Linux_Storage_Stack_Diagram

># day 4