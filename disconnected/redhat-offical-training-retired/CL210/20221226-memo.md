# day 1

**강사이름:** 최국현
**메일주소:** <bluehelix@gmail.com>


**화이트 보드**

[MS 화이트보드](https://wbd.ms/share/v2/aHR0cHM6Ly93aGl0ZWJvYXJkLm1pY3Jvc29mdC5jb20vYXBpL3YxLjAvd2hpdGVib2FyZHMvcmVkZWVtL2ZmODkwODFkZTJjMDRhOGY5ZDdmZWJhYWVkM2ZjYmYzX0JCQTcxNzYyLTEyRTAtNDJFMS1CMzI0LTVCMTMxRjQyNEUzRF8xMzI3Y2FmZi05NTQ1LTQwZGUtYjM1Mi02YTIyMGVhNmMxYjc=)

**메모 주소**
[메모주소](https://github.com/tangt64/training_memos/blob/main/redhat/CL210/20221226-memo.md)


**점심시간**: 12시부터 1시 20분까지
**쉬는시간**: 15분

## 강의 진행전 준비작업

1. gnome-tweaks
```bash
sudo yum install gnome-tweaks
gnome-tweaks 
```


"heat", "ansible"의 차이 및 공통점
1. YAML사용한다는 부분에서 공통점
2. ansible은 모듈 기반 작업
3. 워크플로우, 변수 값 위주의 작업
4. heat는 사양에 따라서 동작 및 작업을 수행

heat는 기본적으로 YAML기반으로, 렌더링(파싱)후 컴파일 및 실행을 한다.

**OpenStack Heat**: <https://wiki.openstack.org/wiki/Heat>

### undercloud network

1. dhcp ---> dnsmasq(dhcp, dns)             
   ----
   \
    '---> 구성시 사용하는 "A"레코드 생성
2. tft
3. pxeboot

"OVS"는 리눅스 브릿지와 함께 사용이 되고 있으며, OV는(은) 기존 리눅스 브릿지를 랩핑을 하고 있음.
```bash
ip -br a        ## ovs + linux bridge compoment 
ovs-vsctl show
ovs-vsctl br-list  ## ovs bridge list
                   ## ovs정보는 ovsdb에서 가져와서 출력

```

```
      .----------------------------.
     /   rabbitmq[message]          \
+--------+                       +--------+
| cinder |  --- (snap=image)---> | glance | 
+--------+               +--------+
    APP                               APP
          RPC(Remote Program Call)
```

## day 2

### docker vs podman

"openstack kolla"는 아직 docker를 사용하고 있음. 
>docker는 containerd로 분리 및 변경
>기존 도커는 더 이상 사용하지 않음
>docker ---> cri-dockerd변경됨
>docker ---> podman 전환 
>레드햇 RHOSP경우에는 podman를 기반으로 되어 있음. 

**podman기반으로 docker호환성 유지**

```bash
yum install podman podman-docker epel-release
yum install docker-compose
```
**정리**
RHOSP는 docker대신 podman를 사용하고 있음.
커뮤니티 경우에는 아직 docker기반으로 사용.
CRI표준을 따름.(이미지 및 명령어).
conmon이 컨테이너 모니터링, runc가 컨테이너 실행


0. podman, buildah, skopeo(from docker functions)
  - search
  - build
  - run
  
1. CRI표준 방식으로 동작하는 경우 어떤구조?
  - podman으로 일반적으로 컨테이너 실행
      + standalone 형식 
      + CRI standard 사용
          * docker image는 현재 산업 표준
          * docker command도 현재 산업 표준
  - conmon기반으로 컨테이너 실행
      + 이전에는 docker == container
          * docker -> dockerd -> containerd -> shim
          * 이 구조는 너무 복잡함
      + 현재는 container == runtime*
          * runc, crun, kata...
      + podman에서 사용하는 데몬(daemon) API서버 역할만 하기 때문에 사용하지 않음
      + 쿠버네티스, cri-docker, podman, crio, containerd를 사용해도 "conmon"기반으로 동작
      + conmon은, container monitoring 모니터링
          * runc가 현재 사용중이 런타임
          * podman, docker, crio(kubernetes)
              - 공통적으로 사용하는 실제 런타임은 "runc"
 
2. podman어떤역할을 해주는가?
  - container, pod, volume, network같은 정보를 관리

3. openstack kolla 이미지 
  - dumb-init 기반으로 동작(init, signal control)
  - rootstate 권한을 더 많이 가지고 있음
      + linux capabilities(EffectiveCaps, RHOSP Kolla는 일반 컨테이너와 다른 권한을 더 가지고 있음)
      + namespace(mnt)
      + 컨테이너에서 사용하는 설정 파일은 바인딩이 되어 있음
          * podman inspect
          * mount -obind 
          * namespace <mnt>자원을 통해서 다시 컨테이너로 전달
          
4. rootless vs rootstate(ful) 컨테이너
  - kolla vs normal container

### about kolla


RHOSP에서는 이미지 빌드(build scratch)하지 않음
```
docker ---> podman
              \
               '---> centos ---> rocky
                     ------
                        |
                        v
                      RHEL

[director]
    |         .-------------------.
    |        /        |            \
    |       /         v             \
**kolla**(ansible+baremetal(container_template(image_build)))
                            ------------------   /
                                   \            /
                                    '----------'
```

### OpenStack(kolla)

roles: 특정 기능을 묶어둔 기능파일
  - yaml 덩어리
  - 한국어로 번역이 되면서 "역할" == 기능상 역할(==ansible role)


### ansible inventory(kolla)

```ini
[undercloud]  ## name of server group
srv1.example.com
srv2.example.com
compute0.example.com
```

**mainly backup**
- undercloud
- overcloud
- director
- controller
- compute

**additionally**
- storage



**roles:** 한개 이상의 "role"의 집합

```yaml
roles:
    - role: test1
    - role: test2
```


### tripwire and aide

**system side**
**AIDE:** Advanced Intrusion Detection Environment
  - 시스템 디렉터리 및 파일 변경 모니터링
  - 비-에어전트 시스템
  - crond기반으로 동작 및 검사

**Tripwire**: 현재는 상용으로 변경이 되면서 라이선스 문제로 더 이상 사용하지 않음
  - 하지만, 원하시는 경우 여전히 사용이 가능함
  - 에어전트 시스템 지원
  - 실시간 감사 기능도 제공

1. 라이선스 문제가
2. AIDE전체적인 변경 사항을 확인하고
3. audit, selinux가 나머지 변경 사항들을 실시간으로 추적 및 감시, 감사.


**TLS(SSL)**

Kolla OpenStack(RHOSP)
- FreeIPA(idm)
- TLS-e 구현(인증 그리고 도메인 접근)


**freeipa(idm)**
- ldap, kerberos, dogtag, dns, ntp 통합
- 반드시 사용해야 되는 도구는 아님

**token(idm, keystone)**
1. keystone token for openstack service
2. FreeIPA token for infra service

**policy(idm, keystone)
1. keystone policy for role, project, group, policy.yaml(json)
  1-1. admin, member(_member_) 하드코딩
  1-2. swiftoperator, heat_stack_user...등등은 확장 서비스용
2. FreeIPA policy for domain, kerberos princeple key...

**Catalog(endpoints)**

여러개의 엔드포인를 묶인 경우, 카달로그(1개 이상의 서비스의 internal, external)

```bash
openstack endpoint list
openstack catalog list
```

**OpenStack Cache(redis):** 키-값 형태는 일반적으로 redis에서 캐싱
- flavor
- properties

**Memcached:** 오픈스택에서 자주 사용하는 설정 혹은 데이터 저장
- API 설정
- image 캐시
- 오브젝트(설정파일 및 데이터 파일)


### AIO 설치(kolla기반)
root/centos

```bash
hostnamectl set-hostname kolla-aio.example.com
yum install epel-release         ## 앤서블 패키지
yum install git yum-utils -y    
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
fdisk -l
pvs
pvcreate /dev/sdb
vgcreate cinder-volumes /dev/sdb

vi /etc/default/grub
> GRUB_CMDLINE_LINUX="....biosdevname=0 net.ifnames=0"
grub2-mkconfig -o /etc/grub2.cfg
reboot

yum install python3-pip rust cargo openssl-devel python3-devel python3-virtualenv python3-docker -y
yum groupinstall "Development Tools" -y
dnf module install python39

alternatives --config python
> /usr/bin/python3.9
alternatives --config python3
> /usr/bin/python3.9

## 다시 로그아웃 후 다시 로그인
virtualenv-3 -p /usr/bin/python3.9 osp  ## 이걸로 권장
> /root/osp

source osp/bin/activate   ## 쉘 변수 선언 및 설정
pip3 install setuptools setuptools-rust kolla-ansible docker ansible==6.7.0
pip3 list

# 버전참고, https://pypi.org/project/kolla-ansible/#history

cp -r osp/share/kolla-ansible/etc_examples/kolla/ /etc/
cp osp/share/kolla-ansible/ansible/inventory/* ~

ip a s eth0
echo "192.168.90.47    kolla-aio.example.com" >> /etc/hosts
ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa
ssh-copy-id root@192.168.90.47
ssh-copy-id root@localhost

vi /root/all-in-one
> "%s/localhost/kolla-aio.example.com/g"

vi /etc/kolla/passwords.yml
> keystone_admin_password: openstack  ## dashboard login password for admin

vi globals.yml
> enable_haproxy: "no"
> kolla_internal_vip_address: "192.168.90.55"
kolla-genpwd
kolla-ansible install-deps                    ## 앤서블 겔럭시에서 필요한 roles를 다운로드 받음(baremetal)
kolla-ansible -i all-in-one bootstrap-servers 
kolla-ansible -i /root/all-in-one prechecks   ## stream-8 오류가 발생하니 stream-9으로 해주세요.
kolla-ansible -i /root/all-in-one pull  && kolla-ansible -i /root/all-in-one deploy
kolla-ansible -i /root/all-in-one deploy
kolla-ansible -i /root/all-in-one post-deploy
kolla-ansible -i /root/all-in-one genconfig

## mariadb문제가 있는 경우 다음처럼 docker 정리후 다시 "deploy"시작
docker stop $(docker ps -qa)
docker rm $(docker ps -qa)
docker volume rm mariadb
```

### fernet-key

```
keystone --- fernetkey
             --------
              \
               '---> /var/lib/config-data/
                     --------------------
                          workflow
                              ^
                               \
                                '---> mistral

curdini: INI파일 수정 및 조회하는 기능
-------
   \
    '---> vi를 사용하셔도 됨.                                

```





```                                                          .--- "INFRA"도메인에 가입
                                                            /
                                                       -----
openstack project create test_project --project-domain INFRA
                         ------------
                         \
                          '---> "default"도메인 가입

 +--------------+ 
 | test_project |  * vCPU: 10    <--- openstack quota set
 +--------------+  * vMEM: 20GiB
    \
 --parent: 자식 프로젝트 생성 + 쿼터(quota)도 같이 상속
      \
       +--- app_dev
       +- infra_arch    ## CHILD projects
       `- storage_arch


```    

오픈스택 소스코드에 하드코딩 되어있는 역할은 다음과 같음.
1. admin
2. \_member\_, member

- openstack에서 role(역할)은, 이전에는 "policy.json"으로 생성 및 관리 하였음
  + <https://docs.openstack.org/oslo.policy/latest/admin/policy-json-file.html>
- role은 현재는 "policy.yaml"으로 변경이 되었음
  + <https://docs.openstack.org/ocata/config-reference/policy-yaml-file.html>
  
            ```
                  role
               /       \
              / binding \
             /           \
           user      project
           ```
<https://docs.openstack.org/python-openstackclient/queens/cli/command-objects/role.html>           

```bash
                openstack assignment list --names
                -> role + user + project 
                                                                                 server_www_1(CPU: 10% usage)
                                                                                     ^
                                                                                     |
                                                                              ---------------
                                                .--- openstack project create service-billing
                                               /
                                        .---assign---.
                                       /              \
     policy.txt(YAML)                ---> RBAC(oslo) <--- USER = "eng_user1"
("os_compute_api": "role:ktinfra")        ---------
   /servers/v1/get_cpus                     ROLE
                                openstack role create 'ktinfra'

   설정파일
   -------
   /var/lib/config-data/puppet-generated/nova/etc/nova/
                                         glance/
                                         cinder/

   마이그레이션 도구: oslopolicy-convert-json-to-yaml                                         
```                      


# day 3

## 디스크 이미지 비교


**raw:** 속도가 빠름. 대신, 기능이 제한적으로 제공
**qed:** 
**qcow2/3:** 속도는 raw에 비해서는 느림. QCow3가 나오면서 RAW비슷하게 속도가 나옴. 장점은 다양한 기능을 제공함.

[qcowe사양](https://wiki.qemu.org/Features/Qcow3)

>The proposal for QCOW3 is different: It includes a version number increase in order to introduce some incompatible features, however it's strictly an extension of QCOW2 and keeps the fundamental structure unchanged, so that a single codebase will be enough for working with both QCOW2 and QCOW3 images. 

> Near-raw performance, competitive with QED.
> Fully QCOW2 backwards-compatible feature set.
> Extensibility to tackle current and future storage virtualization challenges.



## 이미지 빌드 과정

- openstack
- libvirtd
- qemu(qemu-ga)
- kvm
  + qemu/kvm
- libguestfs-tools-c 
  + 이 도구는 수동으로 명령어 수행

**oz**
이미지 빌드 도구. 사용자가 명시한 순서대로 작업을 진행
템플릿(명령어 중심으로)으로 자동 빌드

**disk-image-builder**
현재 레드햇 경우에는 OpenStack 및 RHV에서 disk-image-builder기반으로 이미지 빌드
자동화 하는 도구. 프레임 워크 디렉터리 및 운영체제 템플릿 기반으로 빌드

**virt-builder**
저장소 형식의 이미지 관리 및 배포도 도구. virt-builder명령어를 통해서 이미지 수정이 가능

### 디스크 이미지 수정

1. 이미지가 sparesify가 된 상태가 아니여야 함.
2. guestfish라는 도구 기반으로 이미지 부트 및 마운트
  - 커널 이미지 + 램 디스크 다운로드 받아서 읽기전용 혹은 수정 가능한 상태로 마운트
3. libvirt, qemu도구가 설치가 되어 있어야 됨. 

```
  [debug]
+-----------+
| guestfish | ---> ramdisk/vmlinuz --->  qemu    ---> disk  ---> <CONSOLE>
+-----------+        [download]        [boot-up]     [mount]

ps -ef | grep qemu


1. virt-builder
2. diskimage-builder ---> [VM_DISK] ---> clean-up    ---> sealed  --->     <UPLOAD>   --->  [GLANCE]
3. oz                                    ---------       --------           ------
                                         once ran       virt-sysprep     virt-sparesify

 


                                                    [vendor-data]
                                                          |
                                                          |
    +----+                     <ovn_router>               |          
    | VM |---| NIC |    ----      OVN     ----       cloud-init  --- [user-data]
    +----+                     [169.254.169.254]          |
      |                         PORT: 8775                |
      |                                                   |
+------------+                                      [meta-data]     
| cloud-init |                                                 
+------------+

```

## OVN/OVS

- 기존 네이티브(레거시) 기반보다 빠르게 확장 및 복구가 가능
- 사용자가 손쉽게 자원 상태를 조회 및 추적이 가능

1. OVN(part of NFV)
  + userspace mode
  + iptables/nftables
  + namespace device
  + S/D NAT
  + ACL(iptables chain)
  + network resource
    * route
    * port
    * securitygroup
  + ovn-northd, ovn-southd  ## 장점

2. OVS(part of NFV, opendaylight, openflow)    
  + linux bridge
  + wrapper with netfilter 
  + OVSPort, VM Device
  + userspace mode
  + ovsdb                   ## 장점
  
 

```                                           
                                                                          <namespace>
                                                                          .----------.
                                                                         /            \
  +-----+                                                               +-----+      +-----+      +----+
  | ML2 |  --->    OVS    --->   [OVS_BRIDGE]  ---> [Linux Bridge] <--- | TAP | <--- | TAP | <--- | VM |
  +-----+                          <Bridge>            <Bridge>         +-----+      +-----+      +----+  
  <plugin>                           /                                       
                                    /
        OVSDB ---> ovs_vswitched --'                              

   +------+        +------+
   |  VM  |        |  VM  | [ip_address]
   +------+        +------+                                                                         (flat)
       |               |                                                                              ^
     [tap]           [tap]                                                                            |  .--- (Masquerading) ---> (FIP) 
       |               |                                                                              | /
       | vlan1         | vlan1                                                                        |/
     [port]          [port]                                                                         [NAT]
       |               |                                                              [dhcp]          |
       |               |                                                                 |            |
+------|---------------|----------------+                                                |            |
|   [+DHCP]         [+DHCP]             |                                                |            |
|   [+DNS]          [+DNS]              | [+netfilter]                                   v            |
| [route/switch]  [route/switch]     ---> [namespace,kernel] ===> [VM] --- <PORT> --- [ROUTER] --- <PORT> 
|    [+ACL]          [+ACL]          ---> [netfilter,kernel]                          [SWITCH]
|      |               |                |                        
|   [network]        [network]       ---> [namespace,kernel]
|      |               |                |
+----[OVN, Open Virtual Network]--------+   
       |               |
       |               | 
     [port]          [port]
       |               |
       |               |
       v               v
+---[OVS, Open Virtual Switch, BRIDGE]---------------------------------+
            |   | |
            |   | |
            |   | |
            |   |  `--- <BR-EX> ---> <EXTERNAL> 
            |   |     .--- [OVN] 
            |   |    /    
            |   v   v
            | <BR-INT>                                      <BR-INT>
            |      \ [NODE-1]                             / [NODE-2]
            |       '------------<============>----------'
            |                      GENEVE(vxlan + gre)
            v
           <BR-TRUNK + VLAN> 
                - storage(tag,external)
                - Internal API(tag,external)
                - Management(tag,external)
                - Teanent Network(tag,external)


                 +-----+
   kernel < ---- | TAP | ----> user
                 +-----+
            protect mode(x86)
            virtio driver(para-driver)
            
              (# ovs-vsctl show)
                     |
                     |             .---(# virsh dominflist <VM_ID>)
                     |            /
bridge link == OVS Port(link) == TAP == VM
-----------                      ---------
(replaced)                      [MAC Learn]
  # bridge fdb MAC \                  /
                    >----------------'
  # ovn        ARP /

```  
## Geneve의 장점



아래와 같이 "vxlan", "gre"의 장점을 가져옴.
**vxlan**
1. 엔드포인트 설정을 자동으로 함. 
2. vxlan도 엔드포인트 설정을 자동으로 함. 

**gre**
1. 대용량 데이터 전송
2. 빠른 속도

**vlan**
- 가상머신 영역에서 구성 및 동작
- Linux Bridge, OVS, OVN

OVN, MASTER ---> CLIENT
ovn-controller.service

OVS, MASTER ---> CLIENT
-----------      ------
controller0      compute0,1


# day 4(day 3이어서..)

### 가상머신 구성시 필요 사항

1. 이미지 필요(qcow) [v] (qcow ---> raw)
2. OVN/OVS
  - 네트워크(서브넷 네트워크 필요) [v]
  - 라우터(외부 혹은 다른 네트워크 통신) [v]
4. libvirtd
  - /var/lib/libvirt/images  [v]
  - libvirt/images ---> ceph rbd [v]
  - /var/lib/nova/instances  [v]
    + \_base, 여기에 저장되는 파일은 raw변환된 확장된 glance이미지(/var/lib/nova/instances/\_image) [v]
    + 이 이외 디렉터리 경우에는 "backing file"이며, 실제 instance가 사용하는 이미지 [v]
5. qemu
6. block service(netapp, optional)
  - block
  - Instance(VM) OS image ---> block(netapp)

```
[CEPH RBD]
     ^         { console_log }              .---> QEMU/KVM
     |               ^                     /
{backingfile}        |                [DRIVER] ---> CEPH STORAGE(RBD)
     |               |                   /     ---> OVS NETWORK(BRIDGE)  
+----------+     +----------+     +-----+--------+ 
| Instance | --- | libvirtd | --- | nova-compute | <--- # openstack server list
+----+-----+     +----------+     +---+----------+      
     |               \    \             \
     |                \    '--------- [DRIVER]
  [virtio]             \                  \
     |                  \                  '---> /var/lib/nova/instances [o]
     |                   '---> /var/lib/libvirt/images/ [x]
   [tap]                       /etc/libvirt/qemu/       [INSTANCE .xml]  # openstack server create
     |
     |    # virsh domiflist <ID>
+----+---------+
| L2/L3 Plugin |     
+----+---------+
     |
[OVS_BRIDGE]
     |
   [PORT] ---.
     |       | 
   [OVN]     | { OVS }
     |       |
   [PORT] ---'

```


```
 # openstack migrate --live <INSTANCE> 
                +------------+                                              +--------------+
                |  instance  |                  ====>                       | new_instance |
                +------------+                                              +--------------+               +------------------+
                { qemu-kvm }  ===> { memory_access }   ===>  copy ===>     {new_guest_memory}              |  new_instance    |
                                     [GUEST_MEMORY]                                /                       +------------------+ 
                                                                                  /                        { new_guest_memory }
                +------------+   ===> shared  ===> unchaged ===> migrated ===> attach                            /
                |   storage  |        {CEPH}                                                                    /
                +------------+   ===> block   ===> migrating ===> keepalive ===> chanaged ===> migrated  ===> attache 
                { libvirtd }          {NFS}
```

**evacuate:** 스토리지나 네트워크 환경이 마이그레이션이 가능한 환경이어야 함.
**migrate/migration**
- **libvirtd**
  + 대다수 마이그레이션 기능을 libvirtd에서 제어 및 관리
  + daemon to daemon으로 동작
  + migreation은 일반적으로 libvirtd기반으로 수행 
  + 보안상 이유때문에...
- **qemu.conf**
  + migrate_라는 이름으로 마이그레이션 기능이 선언이 되어 있음
  + 
- nova.con
  + /var/lib/config-data/puppet-generated/nova_libvirt/etc/nova/nova.conf



### 스토리지

- lvm, kolla기본 스토리지로 "cinder-volumes"
- RHOSP경우에는 CEPH기반으로 사용

### 추가 설명 할 부분(내일 아침에 추가 설명)

- podman 구성 및 구조 [v]
  * 오픈스택에서 어떤식으로 사용하는지?
  * 기존과 차이점?

- rabbitmq dashboard에서 topic, queue[v]
- AIO(All-in-one)설치 최종으로 정리하기[v]
- yum ---> dnf, module pakcage[ ]
